{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c1l1mo/.local/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn,utils\n",
    "import lightning as L\n",
    "from transformer.transformer import CARL\n",
    "import sys\n",
    "sys.path.append(\"/home/c1l1mo/projects/VideoAlignment/\")\n",
    "from loss.scl import SCL\n",
    "import yaml\n",
    "from easydict import EasyDict as Edict\n",
    "from dataset import construct_dataloader\n",
    "from dataset.penn_action import PennAction\n",
    "import os\n",
    "\n",
    "with open(\"/home/c1l1mo/projects/VideoAlignment/result/scl_penn_action/config.yaml\", 'r') as config_file:\n",
    "    config_dict = yaml.safe_load(config_file)\n",
    "cfg = Edict(config_dict)\n",
    "cfg.PATH_TO_DATASET = os.path.join(\"/home/c1l1mo/datasets\",cfg.PATH_TO_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "carl = CARL(cfg)\n",
    "scl = SCL(cfg)\n",
    "\n",
    "\n",
    "class LitCARL(L.LightningModule):\n",
    "    def __init__(self,carl,scl):\n",
    "        super().__init__()\n",
    "        self.carl=carl\n",
    "        self.scl =scl\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        original_video,video,label,seq_len,steps,mask,name,skeleton = batch\n",
    "        batch_size, num_views, num_steps, c, h, w = video.shape\n",
    "        video = video.view(-1, num_steps, c, h, w)\n",
    "        embs = self.carl(video,video_mask=mask,skeleton=skeleton)\n",
    "        loss = self.scl.compute_loss(embs,seq_len.to(embs.device),steps.to(embs.device),mask.to(embs.device))\n",
    "        self.log(\"train_loss: \" , loss)\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=cfg.OPTIMIZER.LR.INITIAL_LR,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decay=cfg.OPTIMIZER.WEIGHT_DECAY,)\n",
    "        return optimizer\n",
    "    def train_dataloader(self):\n",
    "        dataset = PennAction(cfg,\"train\",mode=\"train\",algo=\"scl\")\n",
    "        data_loader = utils.data.DataLoader(dataset)\n",
    "        return data_loader\n",
    "litcarl = LitCARL(carl,scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | carl | CARL | 27.4 M\n",
      "------------------------------\n",
      "27.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.4 M    Total params\n",
      "109.554   Total estimated model params size (MB)\n",
      "2023-11-07 15:33:18.151109: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/c1l1mo/.local/lib/python3.7/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|████                                                                                                                                                                                                          | 2/100 [00:13<10:41,  6.55s/it, loss=1.6, v_num=15]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c1l1mo/.local/lib/python3.7/site-packages/lightning/pytorch/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(limit_train_batches=100,max_epochs=1,accelerator='gpu',devices=[0,1])\n",
    "trainer.fit(model=litcarl,train_dataloaders=data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carl",
   "language": "python",
   "name": "carl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

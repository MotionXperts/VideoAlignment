{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the structor is ResNet50 -> Transformation -> Position Encoding -> Transformer Encoder-> MLP head\n",
    "## (B , T , 3 , 224 , 224) -> (B , T , 1024 , 14 , 14) -> (B, T , 2048, 7 , 7) \n",
    "## -> (B * T , 2048 , 1 , 1) \n",
    "## -> (B*T , 2048 ) -> (B*T , 256) -> (B , T , 256) -> (B , T , 256) -> (B , T , 256) -> (B , T , 256) -> (B*T , 256) \n",
    "## -> (B * T , 128) -> (B , T , 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c1l1mo/.local/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/c1l1mo/.local/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/home/c1l1mo/.local/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class ResNet50(nn.Module):\n",
    "    \"\"\"\n",
    "    The resnet50 layer in the carl paper, the resnet is used to extract feature. We freeze all layers prior to -3 \n",
    "        then train the following layers for our use.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Download the pretrain model.\n",
    "        Specify layers to use\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.model.children())[:-3])\n",
    "        self.finetune_layer = nn.Sequential(*list(self.model.children())[-3])\n",
    "        \n",
    "        self.resnet_pool = nn.AdaptiveAvgPool2d(1)\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Use Resnet 50 to extract per-frame features.\n",
    "        -------\n",
    "        Input:\n",
    "            x: (B , T , 3 , 224 , 224)\n",
    "        Output:\n",
    "            out: (B , T , 2048) (The output dimension is the same as the -2 layer of ResNet, the only different\n",
    "            is that we finetune layers between ResNet[-3:-1])\n",
    "        \"\"\"\n",
    "        B , T , C , W , H = x.shape\n",
    "        frames_per_batch = 25 ## Configuration of how many frames resnet can take once.\n",
    "        num_blocks = int(math.ceil(float(T) / frames_per_batch))\n",
    "        output = []\n",
    "        for i in range(num_blocks): \n",
    "            ## make sure the boundary case is considered\n",
    "            if (i+1)*frames_per_batch > T:\n",
    "                processing = x[:, i*frames_per_batch:]\n",
    "            else:\n",
    "                processing = x[:, i*frames_per_batch:(i+1)*frames_per_batch]\n",
    "            print(processing.shape)\n",
    "            processing = processing.contiguous().view(-1,C,W,H)\n",
    "            print(processing.shape)\n",
    "            ## feed into ResNet\n",
    "            self.backbone.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                resnet_frame = self.backbone(processing)\n",
    "            ## append finetune part\n",
    "            finetune_frame = self.finetune_layer(resnet_frame)\n",
    "            \n",
    "            processing = finetune_frame.contiguous().view(B,-1,2048,7,7)\n",
    "            \n",
    "            output.append(processing)\n",
    "        x = torch.cat(output,dim=1)\n",
    "        x = self.resnet_pool(x)\n",
    "        x = x.flatten(start_dim=2)\n",
    "        return x\n",
    "            \n",
    "r = ResNet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c1l1mo/.local/lib/python3.7/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/home/c1l1mo/.local/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 66, 3, 224, 224])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import Resize,functional\n",
    "x,_,_ = read_video(\"/home/c1l1mo/datasets/ACM_skating/Axel_and_Axel_com/467204328706015300.mp4\")\n",
    "y,_,_ = read_video(\"/home/c1l1mo/datasets/ACM_skating/Axel_and_Axel_com/467204328706015300.mp4\")\n",
    "x = torch.tensor(torch.cat((x.unsqueeze(0),y.unsqueeze(0)),dim=0)).float()\n",
    "x = x[:,:,:224,:224]\n",
    "x = x.permute(0,1,4,2,3)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 25, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([2, 25, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([2, 16, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([2, 66, 2048])\n"
     ]
    }
   ],
   "source": [
    "resnet_out = r(x)\n",
    "print(resnet_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_layer = []\n",
    "        \n",
    "        inchannel = 2048\n",
    "        for layer in range(2):\n",
    "            self.fc_layer.append(nn.Dropout(0.1))\n",
    "            self.fc_layer.append(nn.Linear(inchannel,512))\n",
    "            self.fc_layer.append(nn.BatchNorm1d(512))\n",
    "            self.fc_layer.append(nn.ReLU(True))\n",
    "            inchannel = 512\n",
    "        self.fc_layer = nn.Sequential(*self.fc_layer)\n",
    "        \n",
    "        self.video_emb = nn.Linear(512,256)\n",
    "    def forward(self,x):\n",
    "        B , T , R = x.shape\n",
    "        x = x.view(-1,R)\n",
    "        x = self.fc_layer(x)\n",
    "        \n",
    "        x = self.video_emb(x)\n",
    "        x = x.view(B,T , x.shape[1])\n",
    "        return x\n",
    "    \n",
    "t = Transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 66, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = t(resnet_out)\n",
    "transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 66, 256]) torch.Size([1, 66, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 66, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoder,self).__init__() ## There is NO DIFFERENCES b/t super() and super(className,self) AFTER python3\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "    def generate_position_encoding(self,seq_len,d_model):\n",
    "        \"\"\"\n",
    "        Position Encoding:\n",
    "            Generate multiple seq, the dimesion will be (seq_len,d_model)\n",
    "            For even number of dimension \"d_model\", generate sin wave.\n",
    "            For odd numbers, generate cosine wave.\n",
    "        \"\"\"\n",
    "        pos_matrix = np.zeros((seq_len,d_model))\n",
    "        for pos in range(seq_len):\n",
    "            for i in np.arange(d_model/2):\n",
    "                pos_matrix[pos,int(2*i)] = np.sin(pos / 10000**(2*i / d_model))\n",
    "                pos_matrix[pos,int(2*i)+1] = np.cos(pos / 10000**(2*i / d_model))\n",
    "        return torch.from_numpy(pos_matrix).unsqueeze(0)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        B , T , D = x.shape\n",
    "        pos_matrix = self.generate_position_encoding(T,D)\n",
    "        print(x.shape,pos_matrix.shape)\n",
    "        x = x + pos_matrix.type_as(x)\n",
    "        x = self.drop_out(x)\n",
    "        return x\n",
    "\n",
    "PE = PositionalEncoder()\n",
    "pe = PE(transformed)\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner_product shape torch.Size([2, 32, 66, 66])\n",
      "torch.Size([2, 66, 32, 8])\n",
      "torch.Size([2, 66, 256])\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,embed_size,heads):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = self.embed_size // self.heads\n",
    "        assert self.head_dim * self.heads == self.embed_size, \"dim not compatible\"\n",
    "        \n",
    "        self.Q2d = nn.Linear(embed_size,embed_size)\n",
    "        self.K2d = nn.Linear(embed_size,embed_size)\n",
    "        self.V2d = nn.Linear(embed_size,embed_size)\n",
    "        \n",
    "        self.d2o = nn.Linear(embed_size,embed_size)\n",
    "        \n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "    def forward(self,Q,K,V,mask= None):\n",
    "        ## in essence, Q, K and V comes from the same tensor(which is X). So Q.shape = K.shape = V.shape\n",
    "        B , T , _ = Q.shape\n",
    "        \n",
    "        ## generate embeddings for query, key ,value\n",
    "        Q = self.Q2d(Q)\n",
    "        K = self.K2d(K)\n",
    "        V = self.V2d(V)\n",
    "        ## split QKV to n_heads\n",
    "        Q = Q.view(B , -1 , self.head_dim,self.heads)\n",
    "        K = K.view(B , -1 , self.head_dim,self.heads)\n",
    "        V = V.view(B , -1 , self.head_dim,self.heads)\n",
    "        ## do inner-product for queries and keys\n",
    "        inner_product = torch.einsum(\"bqhd,bkhd->bhqk\",[Q,K])\n",
    "        print(\"inner_product shape\" , inner_product.shape)\n",
    "        ## apply mask in case some of the inputs are padded instead of real things\n",
    "        if mask is not None:\n",
    "            inner_product = inner_product.mask_filled(mask==0,float(\"-1e20\"))\n",
    "        ## find how many attention to pay for each place\n",
    "        attention = torch.softmax(inner_product / (self.embed_size**(1/2)),dim=3) ## divided by self.embed_size ^ 1/2 according to the paper\n",
    "        ## sum up the values, multiplied by attention in palce\n",
    "        out = torch.einsum(\"bhqk,bvhd->bqhd\",[attention,V])\n",
    "        print(out.shape)\n",
    "        # out.permute(0,3,1,2)\n",
    "        # print(out.shape)\n",
    "        ## apply drop out\n",
    "        out = self.drop_out(out).reshape(B,T,-1)\n",
    "        \n",
    "        out = self.d2o(out)\n",
    "        print(out.shape)\n",
    "        return out\n",
    "A = Attention(transformed.shape[2],8)\n",
    "a = A(transformed,transformed,transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self,embed_size,d_out):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.drop_out = nn.Dropout(d_out)\n",
    "    def forward(self,x,sublayer):\n",
    "        res = self.norm(x)\n",
    "        res = sublayer(res)\n",
    "        res= self.drop_out(res)\n",
    "        return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner_product shape torch.Size([2, 32, 66, 66])\n",
      "torch.Size([2, 66, 32, 8])\n",
      "torch.Size([2, 66, 256])\n",
      "torch.Size([2, 66, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 66, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncodingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer's encoding layer, in each layer it will do attention mechanism and feedforward\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_size,d_ff=1024):\n",
    "        super().__init__()\n",
    "        self.reslayer = ResidualConnection(embed_size,.1)\n",
    "        self.attention = Attention(embed_size,8)\n",
    "        \n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_size,d_ff),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_ff,embed_size),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        sublayer = lambda x:self.attention(x,x,x,None)\n",
    "        x = self.reslayer(x,sublayer)\n",
    "        print(x.shape)\n",
    "        x = self.reslayer(x,self.feedforward)\n",
    "        return x\n",
    "\n",
    "EL = EncodingLayer(transformed.shape[2])\n",
    "el = EL(a)\n",
    "el.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 66, 256]) torch.Size([1, 66, 256])\n",
      "inner_product shape torch.Size([2, 32, 66, 66])\n",
      "torch.Size([2, 66, 32, 8])\n",
      "torch.Size([2, 66, 256])\n",
      "torch.Size([2, 66, 256])\n",
      "inner_product shape torch.Size([2, 32, 66, 66])\n",
      "torch.Size([2, 66, 32, 8])\n",
      "torch.Size([2, 66, 256])\n",
      "torch.Size([2, 66, 256])\n",
      "inner_product shape torch.Size([2, 32, 66, 66])\n",
      "torch.Size([2, 66, 32, 8])\n",
      "torch.Size([2, 66, 256])\n",
      "torch.Size([2, 66, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 66, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,embed_size):\n",
    "        super().__init__()\n",
    "        self.positional_encoder = PositionalEncoder()\n",
    "        self.encode_layers = nn.ModuleList([\n",
    "            EncodingLayer(embed_size) for _ in range(3)\n",
    "        ])\n",
    "        self.embedding_layer = nn.Linear(256,128)\n",
    "    def forward(self,x):\n",
    "        x = self.positional_encoder(x)\n",
    "        for encode_layer in self.encode_layers:\n",
    "            x = encode_layer(x)\n",
    "        x = self.embedding_layer(x)\n",
    "        return x\n",
    "Transformer = TransformerEncoder(256)\n",
    "tfr = Transformer(transformed)\n",
    "\n",
    "tfr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c1l1mo/.local/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 66, 3, 224, 224])\n",
      "torch.Size([2, 25, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([2, 25, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([2, 16, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([2, 66, 256]) torch.Size([1, 66, 256])\n",
      "inner_product shape torch.Size([2, 32, 66, 66])\n",
      "torch.Size([2, 66, 32, 8])\n",
      "torch.Size([2, 66, 256])\n",
      "torch.Size([2, 66, 256])\n",
      "inner_product shape torch.Size([2, 32, 66, 66])\n",
      "torch.Size([2, 66, 32, 8])\n",
      "torch.Size([2, 66, 256])\n",
      "torch.Size([2, 66, 256])\n",
      "inner_product shape torch.Size([2, 32, 66, 66])\n",
      "torch.Size([2, 66, 32, 8])\n",
      "torch.Size([2, 66, 256])\n",
      "torch.Size([2, 66, 256])\n",
      "torch.Size([2, 66, 128])\n"
     ]
    }
   ],
   "source": [
    "class CARL(nn.Module):\n",
    "    def __init__(self,embed_size):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNet50()\n",
    "        self.transformation = Transformation()\n",
    "        self.transformerEncoder = TransformerEncoder(embed_size)\n",
    "    def forward(self,x):\n",
    "        resnet = self.resnet(x)\n",
    "        transformed = self.transformation(resnet)\n",
    "        encoding = self.transformerEncoder(transformed)\n",
    "        return encoding\n",
    "carl = CARL(256)\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import Resize,functional\n",
    "x,_,_ = read_video(\"/home/c1l1mo/datasets/ACM_skating/Axel_and_Axel_com/467204328706015300.mp4\")\n",
    "y,_,_ = read_video(\"/home/c1l1mo/datasets/ACM_skating/Axel_and_Axel_com/467204328706015300.mp4\")\n",
    "x = torch.tensor(torch.cat((x.unsqueeze(0),y.unsqueeze(0)),dim=0)).float()\n",
    "x = x[:,:,:224,:224]\n",
    "x = x.permute(0,1,4,2,3)\n",
    "print(x.shape)\n",
    "\n",
    "y = carl(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOCA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.phi = nn.Conv1d(2048,1024,kernel_size=1,stride=1)\n",
    "        self.theta = nn.Conv1d(2048,1024,kernel_size=1,stride=1)\n",
    "        self.g = nn.Conv1d(2048,1024,kernel_size=1,stride=1)\n",
    "        self.ruo = nn.Conv2d(2,1,kernel_size=1,stride=1)\n",
    "        self.W = nn.Conv1d(1024,2048,kernel_size=1,stride=1)\n",
    "        self.video_emb=nn.Linear(2048,256)\n",
    "    def forward(self,x):\n",
    "        print(x.shape)\n",
    "        x = x.permute(0,2,1)\n",
    "        B, D, T = x.size()\n",
    "        ## NSSM\n",
    "        x_ = x.permute(0,2,1)\n",
    "        NSSM = x_.matmul(x).softmax(dim=-1)\n",
    "        print(\"NSSM shape:\" , NSSM)\n",
    "        # AttentionMap\n",
    "        x_theta = self.theta(x)\n",
    "        x_phi = self.phi(x).permute(0,2,1).contiguous()\n",
    "        AttentionMap = x_phi.matmul(x_theta).softmax(dim=-1)\n",
    "        print(\"Attention shape:\" , AttentionMap)\n",
    "        # MocaMap\n",
    "        x_concat = torch.cat((NSSM,AttentionMap),dim=0)\n",
    "        print(\"original x concat shape:\" , x_concat.shape)\n",
    "        x_concat = x_concat.view(NSSM.size(0),-1,NSSM.size(1),NSSM.size(2))\n",
    "        print(\"x concat shape: \" , x_concat.shape)\n",
    "        MocaMap = self.ruo(x_concat).reshape(B,T,T)\n",
    "        print(\"MocaMap shape:\" , MocaMap.shape)\n",
    "        # g branch\n",
    "        x_g = self.g(x).permute(0,2,1)\n",
    "        print(\"X G shape:\" , x_g.shape)\n",
    "        \n",
    "        Y = MocaMap.matmul(x_g).permute(0,2,1)\n",
    "        Wz = self.W(Y)\n",
    "        Z = Wz+x\n",
    "        Z = Z.reshape(B,T,D)\n",
    "        Z = self.video_emb(Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 2048])\n",
      "NSSM shape: torch.Size([2, 10, 10])\n",
      "Attention shape: torch.Size([2, 10, 10])\n",
      "original x concat shape: torch.Size([4, 10, 10])\n",
      "x concat shape:  torch.Size([2, 2, 10, 10])\n",
      "MocaMap shape: torch.Size([2, 10, 10])\n",
      "X G shape: torch.Size([2, 10, 1024])\n",
      "Z shape: torch.Size([2, 10, 256])\n",
      "Z : tensor([[[ 3.6941e-01,  4.1282e-01,  3.5105e-01,  ...,  3.9634e-01,\n",
      "          -8.2744e-01, -2.8974e-01],\n",
      "         [ 3.7809e-01,  1.4918e-01,  5.5495e-01,  ...,  3.8702e-01,\n",
      "          -4.4206e-01,  3.2910e-01],\n",
      "         [ 6.5945e-01, -4.2198e-01, -3.2817e-02,  ...,  9.3894e-01,\n",
      "          -4.6161e-04, -3.7253e-01],\n",
      "         ...,\n",
      "         [-1.0380e+00, -1.6090e-01,  5.5413e-01,  ...,  3.9025e-01,\n",
      "           1.3534e+00, -2.0494e-01],\n",
      "         [-5.8026e-01, -1.1518e-01,  2.5479e-01,  ...,  1.2233e+00,\n",
      "          -3.6950e-01, -1.0978e-02],\n",
      "         [-1.3043e-01,  1.1606e-01,  4.8518e-02,  ...,  6.7899e-01,\n",
      "          -3.6790e-01, -5.2244e-01]],\n",
      "\n",
      "        [[ 1.1995e-01, -4.3977e-03,  1.3517e-01,  ...,  3.3773e-01,\n",
      "          -6.5177e-01, -2.0888e-01],\n",
      "         [ 2.3996e-01,  3.2793e-03,  6.4378e-01,  ...,  1.8777e-01,\n",
      "           1.9889e-01,  2.2093e-01],\n",
      "         [ 6.2095e-01, -4.1042e-01,  2.0421e-01,  ...,  1.0591e+00,\n",
      "          -3.0717e-01, -2.0830e-01],\n",
      "         ...,\n",
      "         [-1.0103e+00, -2.8630e-01,  7.9441e-01,  ...,  1.2029e-01,\n",
      "           1.0538e+00, -3.4484e-01],\n",
      "         [-8.6692e-01, -4.1556e-01,  3.6683e-01,  ...,  1.2399e+00,\n",
      "          -5.1281e-01,  6.0059e-02],\n",
      "         [-2.8864e-01,  4.5967e-03,  7.1375e-02,  ...,  7.4329e-01,\n",
      "          -7.7914e-01, -7.4662e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand(2,10,2048)\n",
    "m = MOCA()\n",
    "Z = m(data)\n",
    "print(\"Z shape:\" , Z.shape)\n",
    "print(\"Z :\" , Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_carl = CARL(256)\n",
    "new_carl.transformation = MOCA2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 720, 404, 3])\n",
      "torch.Size([66, 720, 404, 3])\n",
      "torch.Size([2, 66, 3, 224, 224])\n",
      "torch.Size([2, 25, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c1l1mo/.local/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == \"\":\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 25, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([2, 16, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "NSSM shape: tensor([[[1.1842e-08, 2.2539e-16, 1.0000e+00,  ..., 9.1323e-36,\n",
      "          2.8325e-35, 1.3968e-34],\n",
      "         [6.9568e-16, 2.5110e-14, 1.0000e+00,  ..., 2.3888e-36,\n",
      "          2.4585e-36, 2.4070e-35],\n",
      "         [2.1654e-25, 7.0157e-26, 1.0000e+00,  ..., 1.3901e-42,\n",
      "          2.4657e-41, 2.4158e-40],\n",
      "         ...,\n",
      "         [4.3682e-27, 3.7019e-28, 3.0694e-09,  ..., 4.6514e-07,\n",
      "          5.2711e-06, 1.4452e-05],\n",
      "         [1.0525e-27, 2.9599e-29, 4.2313e-09,  ..., 4.0950e-07,\n",
      "          5.2761e-03, 3.6984e-02],\n",
      "         [2.2724e-28, 1.2687e-29, 1.8149e-09,  ..., 4.9152e-08,\n",
      "          1.6191e-03, 7.5612e-01]],\n",
      "\n",
      "        [[1.1842e-08, 2.2539e-16, 1.0000e+00,  ..., 9.1323e-36,\n",
      "          2.8325e-35, 1.3968e-34],\n",
      "         [6.9568e-16, 2.5110e-14, 1.0000e+00,  ..., 2.3888e-36,\n",
      "          2.4585e-36, 2.4070e-35],\n",
      "         [2.1654e-25, 7.0157e-26, 1.0000e+00,  ..., 1.3901e-42,\n",
      "          2.4657e-41, 2.4158e-40],\n",
      "         ...,\n",
      "         [4.3682e-27, 3.7019e-28, 3.0694e-09,  ..., 4.6514e-07,\n",
      "          5.2711e-06, 1.4452e-05],\n",
      "         [1.0525e-27, 2.9599e-29, 4.2313e-09,  ..., 4.0950e-07,\n",
      "          5.2761e-03, 3.6984e-02],\n",
      "         [2.2724e-28, 1.2687e-29, 1.8149e-09,  ..., 4.9152e-08,\n",
      "          1.6191e-03, 7.5612e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "Attention shape: tensor([[[0.0440, 0.0276, 0.0323,  ..., 0.0301, 0.0308, 0.0386],\n",
      "         [0.0486, 0.0314, 0.0382,  ..., 0.0348, 0.0345, 0.0434],\n",
      "         [0.0344, 0.0198, 0.0313,  ..., 0.0346, 0.0338, 0.0414],\n",
      "         ...,\n",
      "         [0.0273, 0.0218, 0.0234,  ..., 0.0288, 0.0314, 0.0397],\n",
      "         [0.0288, 0.0217, 0.0242,  ..., 0.0256, 0.0290, 0.0363],\n",
      "         [0.0359, 0.0256, 0.0290,  ..., 0.0261, 0.0295, 0.0359]],\n",
      "\n",
      "        [[0.0440, 0.0276, 0.0323,  ..., 0.0301, 0.0308, 0.0386],\n",
      "         [0.0486, 0.0314, 0.0382,  ..., 0.0348, 0.0345, 0.0434],\n",
      "         [0.0344, 0.0198, 0.0313,  ..., 0.0346, 0.0338, 0.0414],\n",
      "         ...,\n",
      "         [0.0273, 0.0218, 0.0234,  ..., 0.0288, 0.0314, 0.0397],\n",
      "         [0.0288, 0.0217, 0.0242,  ..., 0.0256, 0.0290, 0.0363],\n",
      "         [0.0359, 0.0256, 0.0290,  ..., 0.0261, 0.0295, 0.0359]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([2, 66, 256]) torch.Size([1, 66, 256])\n",
      "inner_product shape torch.Size([2, 32, 66, 66])\n",
      "torch.Size([2, 66, 32, 8])\n",
      "torch.Size([2, 66, 256])\n",
      "torch.Size([2, 66, 256])\n",
      "inner_product shape torch.Size([2, 32, 66, 66])\n",
      "torch.Size([2, 66, 32, 8])\n",
      "torch.Size([2, 66, 256])\n",
      "torch.Size([2, 66, 256])\n",
      "inner_product shape torch.Size([2, 32, 66, 66])\n",
      "torch.Size([2, 66, 32, 8])\n",
      "torch.Size([2, 66, 256])\n",
      "torch.Size([2, 66, 256])\n",
      "torch.Size([2, 66, 128])\n",
      "tensor([[[ 31.9400,  16.5170, -31.2771,  ...,  49.7296,  15.3507,  40.6041],\n",
      "         [ 28.0978, -11.5819, -25.6156,  ...,  34.6026,  30.8206,   9.4920],\n",
      "         [ 25.7822,   1.4217, -21.1945,  ...,  36.3494,  24.4434,  19.2653],\n",
      "         ...,\n",
      "         [ 46.5808,   6.5051, -22.5467,  ...,  39.7302,   2.4042,  26.0857],\n",
      "         [ 53.3398,   4.9233, -17.7948,  ...,  30.1046,  17.1596,   7.9139],\n",
      "         [ 44.1065,   0.1802, -19.3718,  ...,  12.9562,  17.9070,  10.4005]],\n",
      "\n",
      "        [[ 36.6065,   8.3670, -14.6593,  ...,  35.8267,   8.3832,  29.1612],\n",
      "         [ 56.4241, -18.9737, -32.1756,  ...,  13.3643,  15.6719,  14.7520],\n",
      "         [ 33.3460,  21.0411, -20.1208,  ...,  23.6306,  17.8518,  33.9784],\n",
      "         ...,\n",
      "         [ 28.1334,  16.0360,  -5.8950,  ...,  32.0111,  32.1730,  10.9705],\n",
      "         [ 54.5614,   3.2563, -13.5061,  ...,  34.8205,  20.7906,  28.1468],\n",
      "         [ 28.9145,  17.6200,  -2.9116,  ...,  39.5264,  14.4501,  23.7353]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import Resize,functional\n",
    "x,_,_ = read_video(\"/home/c1l1mo/datasets/ACM_skating/Axel_and_Axel_com/467204328706015300.mp4\")\n",
    "y,_,_ = read_video(\"/home/c1l1mo/datasets/ACM_skating/Axel_and_Axel_com/467204328706015300.mp4\")\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "#x = torch.rand(200,224,224,3)\n",
    "#y = torch.rand(200,224,224,3)\n",
    "\n",
    "x = torch.tensor(torch.cat((x.unsqueeze(0),y.unsqueeze(0)),dim=0)).float()\n",
    "x = x[:,:,:224,:224]\n",
    "x = x.permute(0,1,4,2,3)\n",
    "print(x.shape)\n",
    "\n",
    "y = new_carl(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOCA2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.theta = nn.Conv1d(2048 , 1024 , kernel_size=1 , stride=1 )\n",
    "        self.phi = nn.Conv1d(2048 , 1024 , kernel_size=1 , stride=1 )\n",
    "        self.g = nn.Conv1d(2048 , 1024 , kernel_size=1 , stride=1 )\n",
    "        self.rou = nn.Conv2d(2,1,kernel_size=1,stride=1)\n",
    "        self.w = nn.Conv1d(1024,2048,kernel_size=1,stride=1)\n",
    "\n",
    "        self.video_emb = nn.Linear(2048,256)\n",
    "    def forward(self,x,B=2,T=66):\n",
    "        x = x.view(B,T,2048)\n",
    "        # B , T , D = x.size()\n",
    "        ## NSSM\n",
    "        x_ = x.permute(0,2,1) # B,D,T\n",
    "        NSSM = x.matmul(x_).softmax(dim=-1) # B,T,T\n",
    "        print(\"NSSM shape:\" , NSSM)\n",
    "        ## AttetnionMap\n",
    "\n",
    "        x = x.permute(0,2,1) # B,D,T\n",
    "        x_theta = self.theta(x) # B,D/2,T\n",
    "        x_phi = self.phi(x).permute(0,2,1) # B,T,D/2\n",
    "        AttentionMap = x_phi.matmul(x_theta).softmax(dim=-1) # B,T,T\n",
    "        print(\"Attention shape:\" , AttentionMap)\n",
    "        ## MocaMap\n",
    "        x_concat = torch.cat([NSSM,AttentionMap],dim=0) # 2B,T,T\n",
    "        x_concat = x_concat.view(B,-1,T,T) # B,2,T,T\n",
    "        MocaMap = self.rou(x_concat).squeeze(1).softmax(dim=-1) # B,T,T\n",
    "        ## G branch\n",
    "        x_g = self.g(x) # B,D/2,T\n",
    "\n",
    "        # print(\"MocaMap.shape: \",MocaMap.shape)\n",
    "        # print(\"x_g.shape: \",x_g.shape)\n",
    "\n",
    "        Y = x_g.matmul(MocaMap) # B,D/2,T\n",
    "        Wz = self.w(Y)          # B,D,T\n",
    "        Z = (Wz+x).permute(0,2,1)                # B,T,D\n",
    "        Z = self.video_emb(Z)                    # B,T,256\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('/home/c1l1mo/projects/VideoAlignment')\n",
    "import model\n",
    "imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c1l1mo/.local/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/home/c1l1mo/.local/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'MODEL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11906/472867797.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcarl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcarl_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/VideoAlignment/model/carl_transformer/transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mres50_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBASE_MODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLAYER\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres50_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output of layer \"4\" (counting starts from 0): 1024x14x14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_finetune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres50_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'MODEL'"
     ]
    }
   ],
   "source": [
    "cfg=None\n",
    "carl = model.carl_transformer.transformer.TransformerModel(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carl",
   "language": "python",
   "name": "carl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

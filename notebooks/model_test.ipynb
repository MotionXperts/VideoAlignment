{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the structor is ResNet50 -> Transformation -> Position Encoding -> Transformer Encoder-> MLP head\n",
    "## (B , T , 3 , 224 , 224) -> (B , T , 1024 , 14 , 14) -> (B, T , 2048, 7 , 7) \n",
    "## -> (B * T , 2048 , 1 , 1) \n",
    "## -> (B*T , 2048 ) -> (B*T , 256) -> (B , T , 256) -> (B , T , 256) -> (B , T , 256) -> (B , T , 256) -> (B*T , 256) \n",
    "## -> (B * T , 128) -> (B , T , 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class ResNet50(nn.Module):\n",
    "    \"\"\"\n",
    "    The resnet50 layer in the carl paper, the resnet is used to extract feature. We freeze all layers prior to -3 \n",
    "        then train the following layers for our use.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Download the pretrain model.\n",
    "        Specify layers to use\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.model.children())[:-3])\n",
    "        self.finetune_layer = nn.Sequential(*list(self.model.children())[-3])\n",
    "        \n",
    "        self.resnet_pool = nn.AdaptiveAvgPool2d(1)\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Use Resnet 50 to extract per-frame features.\n",
    "        -------\n",
    "        Input:\n",
    "            x: (B , T , 3 , 224 , 224)\n",
    "        Output:\n",
    "            out: (B , T , 2048) (The output dimension is the same as the -2 layer of ResNet, the only different\n",
    "            is that we finetune layers between ResNet[-3:-1])\n",
    "        \"\"\"\n",
    "        B , T , C , W , H = x.shape\n",
    "        frames_per_batch = 25 ## Configuration of how many frames resnet can take once.\n",
    "        num_blocks = int(math.ceil(float(T) / frames_per_batch))\n",
    "        output = []\n",
    "        for i in range(num_blocks): \n",
    "            ## make sure the boundary case is considered\n",
    "            if (i+1)*frames_per_batch > T:\n",
    "                processing = x[:, i*frames_per_batch:]\n",
    "            else:\n",
    "                processing = x[:, i*frames_per_batch:(i+1)*frames_per_batch]\n",
    "            print(processing.shape)\n",
    "            processing = processing.contiguous().view(-1,C,W,H)\n",
    "            print(processing.shape)\n",
    "            ## feed into ResNet\n",
    "            self.backbone.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                resnet_frame = self.backbone(processing)\n",
    "            ## append finetune part\n",
    "            finetune_frame = self.finetune_layer(resnet_frame)\n",
    "            \n",
    "            processing = finetune_frame.contiguous().view(B,-1,2048,7,7)\n",
    "            \n",
    "            output.append(processing)\n",
    "        x = torch.cat(output,dim=1)\n",
    "        x = self.resnet_pool(x)\n",
    "        x = x.flatten(start_dim=2)\n",
    "        return x\n",
    "            \n",
    "r = ResNet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import Resize,functional\n",
    "x,_,_ = read_video(\"/home/c1l1mo/datasets/ACM_skating/Axel_and_Axel_com/467204328706015300.mp4\")\n",
    "y,_,_ = read_video(\"/home/c1l1mo/datasets/ACM_skating/Axel_and_Axel_com/467204328706015300.mp4\")\n",
    "x = torch.tensor(torch.cat((x.unsqueeze(0),y.unsqueeze(0)),dim=0)).float()\n",
    "x = x[:,:,:224,:224]\n",
    "x = x.permute(0,1,4,2,3)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_out = r(x)\n",
    "print(resnet_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_layer = []\n",
    "        \n",
    "        inchannel = 2048\n",
    "        for layer in range(2):\n",
    "            self.fc_layer.append(nn.Dropout(0.1))\n",
    "            self.fc_layer.append(nn.Linear(inchannel,512))\n",
    "            self.fc_layer.append(nn.BatchNorm1d(512))\n",
    "            self.fc_layer.append(nn.ReLU(True))\n",
    "            inchannel = 512\n",
    "        self.fc_layer = nn.Sequential(*self.fc_layer)\n",
    "        \n",
    "        self.video_emb = nn.Linear(512,256)\n",
    "    def forward(self,x):\n",
    "        B , T , R = x.shape\n",
    "        x = x.view(-1,R)\n",
    "        x = self.fc_layer(x)\n",
    "        \n",
    "        x = self.video_emb(x)\n",
    "        x = x.view(B,T , x.shape[1])\n",
    "        return x\n",
    "    \n",
    "t = Transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = t(resnet_out)\n",
    "transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoder,self).__init__() ## There is NO DIFFERENCES b/t super() and super(className,self) AFTER python3\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "    def generate_position_encoding(self,seq_len,d_model):\n",
    "        \"\"\"\n",
    "        Position Encoding:\n",
    "            Generate multiple seq, the dimesion will be (seq_len,d_model)\n",
    "            For even number of dimension \"d_model\", generate sin wave.\n",
    "            For odd numbers, generate cosine wave.\n",
    "        \"\"\"\n",
    "        pos_matrix = np.zeros((seq_len,d_model))\n",
    "        for pos in range(seq_len):\n",
    "            for i in np.arange(d_model/2):\n",
    "                pos_matrix[pos,int(2*i)] = np.sin(pos / 10000**(2*i / d_model))\n",
    "                pos_matrix[pos,int(2*i)+1] = np.cos(pos / 10000**(2*i / d_model))\n",
    "        return torch.from_numpy(pos_matrix).unsqueeze(0)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        B , T , D = x.shape\n",
    "        pos_matrix = self.generate_position_encoding(T,D)\n",
    "        print(x.shape,pos_matrix.shape)\n",
    "        x = x + pos_matrix.type_as(x)\n",
    "        x = self.drop_out(x)\n",
    "        return x\n",
    "\n",
    "PE = PositionalEncoder()\n",
    "pe = PE(transformed)\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,embed_size,heads):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = self.embed_size // self.heads\n",
    "        assert self.head_dim * self.heads == self.embed_size, \"dim not compatible\"\n",
    "        \n",
    "        self.Q2d = nn.Linear(embed_size,embed_size)\n",
    "        self.K2d = nn.Linear(embed_size,embed_size)\n",
    "        self.V2d = nn.Linear(embed_size,embed_size)\n",
    "        \n",
    "        self.d2o = nn.Linear(embed_size,embed_size)\n",
    "        \n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "    def forward(self,Q,K,V,mask= None):\n",
    "        ## in essence, Q, K and V comes from the same tensor(which is X). So Q.shape = K.shape = V.shape\n",
    "        B , T , _ = Q.shape\n",
    "        \n",
    "        ## generate embeddings for query, key ,value\n",
    "        Q = self.Q2d(Q)\n",
    "        K = self.K2d(K)\n",
    "        V = self.V2d(V)\n",
    "        ## split QKV to n_heads\n",
    "        Q = Q.view(B , -1 , self.head_dim,self.heads)\n",
    "        K = K.view(B , -1 , self.head_dim,self.heads)\n",
    "        V = V.view(B , -1 , self.head_dim,self.heads)\n",
    "        ## do inner-product for queries and keys\n",
    "        inner_product = torch.einsum(\"bqhd,bkhd->bhqk\",[Q,K])\n",
    "        print(\"inner_product shape\" , inner_product.shape)\n",
    "        ## apply mask in case some of the inputs are padded instead of real things\n",
    "        if mask is not None:\n",
    "            inner_product = inner_product.mask_filled(mask==0,float(\"-1e20\"))\n",
    "        ## find how many attention to pay for each place\n",
    "        attention = torch.softmax(inner_product / (self.embed_size**(1/2)),dim=3) ## divided by self.embed_size ^ 1/2 according to the paper\n",
    "        ## sum up the values, multiplied by attention in palce\n",
    "        out = torch.einsum(\"bhqk,bvhd->bqhd\",[attention,V])\n",
    "        print(out.shape)\n",
    "        # out.permute(0,3,1,2)\n",
    "        # print(out.shape)\n",
    "        ## apply drop out\n",
    "        out = self.drop_out(out).reshape(B,T,-1)\n",
    "        \n",
    "        out = self.d2o(out)\n",
    "        print(out.shape)\n",
    "        return out\n",
    "A = Attention(transformed.shape[2],8)\n",
    "a = A(transformed,transformed,transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self,embed_size,d_out):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.drop_out = nn.Dropout(d_out)\n",
    "    def forward(self,x,sublayer):\n",
    "        res = self.norm(x)\n",
    "        res = sublayer(res)\n",
    "        res= self.drop_out(res)\n",
    "        return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer's encoding layer, in each layer it will do attention mechanism and feedforward\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_size,d_ff=1024):\n",
    "        super().__init__()\n",
    "        self.reslayer = ResidualConnection(embed_size,.1)\n",
    "        self.attention = Attention(embed_size,8)\n",
    "        \n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_size,d_ff),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_ff,embed_size),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        sublayer = lambda x:self.attention(x,x,x,None)\n",
    "        x = self.reslayer(x,sublayer)\n",
    "        print(x.shape)\n",
    "        x = self.reslayer(x,self.feedforward)\n",
    "        return x\n",
    "\n",
    "EL = EncodingLayer(transformed.shape[2])\n",
    "el = EL(a)\n",
    "el.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,embed_size):\n",
    "        super().__init__()\n",
    "        self.positional_encoder = PositionalEncoder()\n",
    "        self.encode_layers = nn.ModuleList([\n",
    "            EncodingLayer(embed_size) for _ in range(3)\n",
    "        ])\n",
    "        self.embedding_layer = nn.Linear(256,128)\n",
    "    def forward(self,x):\n",
    "        x = self.positional_encoder(x)\n",
    "        for encode_layer in self.encode_layers:\n",
    "            x = encode_layer(x)\n",
    "        x = self.embedding_layer(x)\n",
    "        return x\n",
    "Transformer = TransformerEncoder(256)\n",
    "tfr = Transformer(transformed)\n",
    "\n",
    "tfr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CARL(nn.Module):\n",
    "    def __init__(self,embed_size):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNet50()\n",
    "        self.transformation = Transformation()\n",
    "        self.transformerEncoder = TransformerEncoder(embed_size)\n",
    "    def forward(self,x):\n",
    "        resnet = self.resnet(x)\n",
    "        transformed = self.transformation(resnet)\n",
    "        encoding = self.transformerEncoder(transformed)\n",
    "        return encoding\n",
    "carl = CARL(256)\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import Resize,functional\n",
    "x,_,_ = read_video(\"/home/c1l1mo/datasets/ACM_skating/Axel_and_Axel_com/467204328706015300.mp4\")\n",
    "y,_,_ = read_video(\"/home/c1l1mo/datasets/ACM_skating/Axel_and_Axel_com/467204328706015300.mp4\")\n",
    "x = torch.tensor(torch.cat((x.unsqueeze(0),y.unsqueeze(0)),dim=0)).float()\n",
    "x = x[:,:,:224,:224]\n",
    "x = x.permute(0,1,4,2,3)\n",
    "print(x.shape)\n",
    "\n",
    "y = carl(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOCA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.phi = nn.Conv1d(2048,1024,kernel_size=1,stride=1)\n",
    "        self.theta = nn.Conv1d(2048,1024,kernel_size=1,stride=1)\n",
    "        self.g = nn.Conv1d(2048,1024,kernel_size=1,stride=1)\n",
    "        self.ruo = nn.Conv2d(2,1,kernel_size=1,stride=1)\n",
    "        self.W = nn.Conv1d(1024,2048,kernel_size=1,stride=1)\n",
    "        self.video_emb=nn.Linear(2048,256)\n",
    "    def forward(self,x):\n",
    "        print(x.shape)\n",
    "        x = x.permute(0,2,1)\n",
    "        B, D, T = x.size()\n",
    "        ## NSSM\n",
    "        x_ = x.permute(0,2,1)\n",
    "        NSSM = x_.matmul(x).softmax(dim=-1)\n",
    "        print(\"NSSM shape:\" , NSSM)\n",
    "        # AttentionMap\n",
    "        x_theta = self.theta(x)\n",
    "        x_phi = self.phi(x).permute(0,2,1).contiguous()\n",
    "        AttentionMap = x_phi.matmul(x_theta).softmax(dim=-1)\n",
    "        print(\"Attention shape:\" , AttentionMap)\n",
    "        # MocaMap\n",
    "        x_concat = torch.cat((NSSM,AttentionMap),dim=0)\n",
    "        print(\"original x concat shape:\" , x_concat.shape)\n",
    "        x_concat = x_concat.view(NSSM.size(0),-1,NSSM.size(1),NSSM.size(2))\n",
    "        print(\"x concat shape: \" , x_concat.shape)\n",
    "        MocaMap = self.ruo(x_concat).reshape(B,T,T)\n",
    "        print(\"MocaMap shape:\" , MocaMap.shape)\n",
    "        # g branch\n",
    "        x_g = self.g(x).permute(0,2,1)\n",
    "        print(\"X G shape:\" , x_g.shape)\n",
    "        \n",
    "        Y = MocaMap.matmul(x_g).permute(0,2,1)\n",
    "        Wz = self.W(Y)\n",
    "        Z = Wz+x\n",
    "        Z = Z.reshape(B,T,D)\n",
    "        Z = self.video_emb(Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.rand(2,10,2048)\n",
    "m = MOCA()\n",
    "Z = m(data)\n",
    "print(\"Z shape:\" , Z.shape)\n",
    "print(\"Z :\" , Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_carl = CARL(256)\n",
    "new_carl.transformation = MOCA2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import Resize,functional\n",
    "x,_,_ = read_video(\"/home/c1l1mo/datasets/ACM_skating/Axel_and_Axel_com/467204328706015300.mp4\")\n",
    "y,_,_ = read_video(\"/home/c1l1mo/datasets/ACM_skating/Axel_and_Axel_com/467204328706015300.mp4\")\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "#x = torch.rand(200,224,224,3)\n",
    "#y = torch.rand(200,224,224,3)\n",
    "\n",
    "x = torch.tensor(torch.cat((x.unsqueeze(0),y.unsqueeze(0)),dim=0)).float()\n",
    "x = x[:,:,:224,:224]\n",
    "x = x.permute(0,1,4,2,3)\n",
    "print(x.shape)\n",
    "\n",
    "y = new_carl(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOCA2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.theta = nn.Conv1d(2048 , 1024 , kernel_size=1 , stride=1 )\n",
    "        self.phi = nn.Conv1d(2048 , 1024 , kernel_size=1 , stride=1 )\n",
    "        self.g = nn.Conv1d(2048 , 1024 , kernel_size=1 , stride=1 )\n",
    "        self.rou = nn.Conv2d(2,1,kernel_size=1,stride=1)\n",
    "        self.w = nn.Conv1d(1024,2048,kernel_size=1,stride=1)\n",
    "\n",
    "        self.video_emb = nn.Linear(2048,256)\n",
    "    def forward(self,x,B=2,T=66):\n",
    "        x = x.view(B,T,2048)\n",
    "        # B , T , D = x.size()\n",
    "        ## NSSM\n",
    "        x_ = x.permute(0,2,1) # B,D,T\n",
    "        NSSM = x.matmul(x_).softmax(dim=-1) # B,T,T\n",
    "        print(\"NSSM shape:\" , NSSM)\n",
    "        ## AttetnionMap\n",
    "\n",
    "        x = x.permute(0,2,1) # B,D,T\n",
    "        x_theta = self.theta(x) # B,D/2,T\n",
    "        x_phi = self.phi(x).permute(0,2,1) # B,T,D/2\n",
    "        AttentionMap = x_phi.matmul(x_theta).softmax(dim=-1) # B,T,T\n",
    "        print(\"Attention shape:\" , AttentionMap)\n",
    "        ## MocaMap\n",
    "        x_concat = torch.cat([NSSM,AttentionMap],dim=0) # 2B,T,T\n",
    "        x_concat = x_concat.view(B,-1,T,T) # B,2,T,T\n",
    "        MocaMap = self.rou(x_concat).squeeze(1).softmax(dim=-1) # B,T,T\n",
    "        ## G branch\n",
    "        x_g = self.g(x) # B,D/2,T\n",
    "\n",
    "        # print(\"MocaMap.shape: \",MocaMap.shape)\n",
    "        # print(\"x_g.shape: \",x_g.shape)\n",
    "\n",
    "        Y = x_g.matmul(MocaMap) # B,D/2,T\n",
    "        Wz = self.w(Y)          # B,D,T\n",
    "        Z = (Wz+x).permute(0,2,1)                # B,T,D\n",
    "        Z = self.video_emb(Z)                    # B,T,256\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c1l1mo/.local/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('/home/c1l1mo/projects/VideoAlignment')\n",
    "import model\n",
    "import yaml\n",
    "cfg_file = \"/home/c1l1mo/projects/VideoAlignment/result/scl_processed_axel_trimmed/config.yaml\"\n",
    "with open(cfg_file, 'r') as config_file:\n",
    "    config_dict = yaml.safe_load(config_file)\n",
    "from easydict import EasyDict as Edict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c1l1mo/.local/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/home/c1l1mo/.local/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 8 256 False\n",
      "256 0.0 False\n",
      "256 0.0 False\n",
      "256 0.0 False\n",
      "256 0.0 False\n",
      "256 0.0 False\n",
      "256 0.0 False\n"
     ]
    }
   ],
   "source": [
    "cfg=Edict(config_dict)\n",
    "carl = model.carl_transformer.transformer.TransformerModel(cfg)\n",
    "mine = model.transformer.transformer.CARL(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "carl_res = carl.backbone\n",
    "my_res = mine.resnet50.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.rand(1,10,3,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def carl_process(x):\n",
    "    batch_size, num_steps, c , h ,w = data.shape\n",
    "    frames_per_batch = 40\n",
    "    num_blocks = int(math.ceil(float(num_steps)/frames_per_batch))\n",
    "    backbone_out = []\n",
    "    for i in range(num_blocks):\n",
    "        curr_idx = i * frames_per_batch\n",
    "        cur_steps = min(num_steps-curr_idx, frames_per_batch) ## make sure the next line will not be out of bound\n",
    "        curr_data = x[:, curr_idx:curr_idx+cur_steps]         ## take a batch for resnet to encode (size will be BASRMODEL.BATCH_SIZE or the remainder)\n",
    "        curr_data = curr_data.contiguous().view(-1, c, h, w)\n",
    "        carl.backbone.eval()\n",
    "        with torch.no_grad():\n",
    "            curr_emb = carl.backbone(curr_data)\n",
    "        curr_emb = carl.res_finetune(curr_emb)\n",
    "        _, out_c, out_h, out_w = curr_emb.size()\n",
    "        curr_emb = curr_emb.contiguous().view(batch_size, cur_steps, out_c, out_h, out_w)\n",
    "        backbone_out.append(curr_emb)\n",
    "        x = torch.cat(backbone_out, dim=1)\n",
    "        _,_,c,h,w = x.shape\n",
    "        x = x.view(batch_size*num_steps,c,h,w)\n",
    "        x = carl.embed.pooling(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        \n",
    "    return x\n",
    "def my_process(x):\n",
    "    x = mine.resnet50(x)\n",
    "    return x\n",
    "    B , T , C , W ,H = x.shape\n",
    "    frames_per_batch = 40\n",
    "    num_blocks = int(math.ceil(float(T)/frames_per_batch))\n",
    "    output = []\n",
    "    for i in range(num_blocks):\n",
    "        if (i+1) * frames_per_batch > T:\n",
    "            processing = x[:,i*frames_per_batch:]\n",
    "        else:\n",
    "            processing = x[:,i*frames_per_batch:(i+1)*frames_per_batch]\n",
    "        processing = processing.contiguous().view(-1,C,W,H)\n",
    "        ## feed into resnet\n",
    "        mine.resnet50.backbone.eval()\n",
    "        with torch.no_grad():\n",
    "            processing = mine.resnet50.backbone(processing)\n",
    "        processing = mine.resnet50.finetune(processing)\n",
    "        processing = processing.view(B,-1,2048,7,7)\n",
    "        output.append(processing)\n",
    "        x = torch.cat(output, dim=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([10, 2048])\n",
      "torch.Size([10, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(torch.equal(carl_process(data),my_process(data)))\n",
    "print(carl_process(data).shape)\n",
    "print(my_process(data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "seed = 7 \n",
    "random.seed(seed)                          \n",
    "np.random.seed(seed)                       \n",
    "torch.manual_seed(seed)                    \n",
    "torch.cuda.manual_seed(seed)               \n",
    "torch.cuda.manual_seed_all(seed)           \n",
    "torch.backends.cudnn.deterministic = True  \n",
    "\n",
    "C = []\n",
    "in_channel = 2048\n",
    "for layer in range(2):\n",
    "    #C.append(nn.Dropout(.1))\n",
    "    C.append(nn.Linear(in_channel,512))\n",
    "    C.append(nn.BatchNorm1d(512))\n",
    "    C.append(nn.ReLU(True))\n",
    "    in_channel = 512\n",
    "C = nn.Sequential(*C)\n",
    "\n",
    "\n",
    "M = []\n",
    "in_channel = 2048\n",
    "for layer in range(2):\n",
    "    #M.append(nn.Dropout(.1))\n",
    "    M.append(nn.Linear(in_channel,512))\n",
    "    M.append(nn.BatchNorm1d(512))\n",
    "    M.append(nn.ReLU(True))\n",
    "    in_channel = 512\n",
    "M = nn.Sequential(*M)\n",
    "    \n",
    "for name,parameter in C.named_parameters():\n",
    "    parameter.data.fill_(.1)\n",
    "for name,parameter in M.named_parameters():\n",
    "    parameter.data.fill_(.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(C(carl_process(data)),M(my_process(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sincos_embedding(seq_len, d_model, train_len=None):\n",
    "    odds = np.arange(0, d_model, 2)\n",
    "    evens = np.arange(1, d_model, 2)\n",
    "    pos_enc_mat = np.zeros((seq_len, d_model))\n",
    "\n",
    "    # if train_len is None:\n",
    "    #     pos_list = np.arange(seq_len)\n",
    "    # else:\n",
    "    #     pos_list = np.linspace(0, train_len-1, num=seq_len)\n",
    "    pos_list = np.arange(seq_len)\n",
    "\n",
    "    for i, pos in enumerate(pos_list):\n",
    "        pos_enc_mat[i, odds] = np.sin(pos / (10000 ** (odds / d_model)))\n",
    "        pos_enc_mat[i, evens] = np.cos(pos / (10000 ** (evens / d_model)))\n",
    "\n",
    "    return torch.from_numpy(pos_enc_mat).unsqueeze(0)\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, dout_p, seq_len=3660):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dout_p)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S, d_model = x.shape\n",
    "        if S != self.seq_len:\n",
    "            pos_enc_mat = generate_sincos_embedding(S, d_model, self.seq_len)\n",
    "            x = x + pos_enc_mat.type_as(x)\n",
    "        else:\n",
    "            pos_enc_mat = generate_sincos_embedding(S, d_model)\n",
    "            x = x + pos_enc_mat.type_as(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,embed_size,dout_p):\n",
    "        super().__init__()\n",
    "        self.embed_size=embed_size  # dimension of the input/output embedding space e.g.: if the input is (T x 256), T is the seqence length and 256 is the embedding space (512)\n",
    "        self.dropout = nn.Dropout(dout_p)  \n",
    "\n",
    "    def encode(self,seq_len,embed_size,train_len=None):\n",
    "        # construct all the odds entries\n",
    "        odds = np.arange(0,embed_size,2)  ## [0 , 2 , 4 , .... , d_model ]    (if d_model is odd) \n",
    "        evens = np.arange(1,embed_size,2) ## [1 , 3 , 5 , .... , d_model-1]   (if d_model is odd)\n",
    "        \n",
    "        # construct multiple positional encoding since transformer operates parrellally\n",
    "        pos_enc_mat = np.zeros((seq_len,embed_size)) ## Shape: (seq_len , d_model)\n",
    "\n",
    "        pos_list = np.arange(seq_len) ## [0 , 1 , 2 , 3 , 4 , .... , seq_len-1]\n",
    "        for i,pos in enumerate((pos_list)):\n",
    "            pos_enc_mat[i, odds]  = np.sin(pos / (10000 ** (odds / embed_size))) \n",
    "            pos_enc_mat[i, evens] = np.cos(pos / (10000 ** (evens / embed_size)))\n",
    "\n",
    "        return torch.from_numpy(pos_enc_mat).unsqueeze(0) \n",
    "    \n",
    "    def forward(self,x):\n",
    "        B , T,embed_size = x.shape \n",
    "        pos_enc_matrix = self.encode(T,embed_size) \n",
    "        x = x + pos_enc_matrix.type_as(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "pe_data = torch.rand(2,10,256)\n",
    "carl_pe = PositionalEncoder(256,0)\n",
    "my_pe = PositionalEncoding(256,0)\n",
    "torch.equal(carl_pe(pe_data),my_pe(pe_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        \n",
    "        for name,parameter in self.fc1.named_parameters():\n",
    "            parameter.data.fill_(.1)\n",
    "        for name,parameter in self.fc2.named_parameters():\n",
    "            parameter.data.fill_(.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''In, Out: (B, S, D)'''\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "embed_size = d_model = 256\n",
    "carl_ff = PositionwiseFeedForward(256,256*4)\n",
    "my_ff = nn.Sequential(\n",
    "            nn.Linear(embed_size,4 * embed_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4 * embed_size,embed_size)\n",
    "        )\n",
    "for name,parameter in my_ff.named_parameters():\n",
    "    parameter.data.fill_(.1)\n",
    "ff_data = torch.rand(2,10,256)\n",
    "torch.equal(carl_ff(ff_data),my_ff(ff_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,embed_size,dout_p,heads=8 , test = False):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.n_heads = embed_size // heads\n",
    "        assert self.embed_size % self.heads == 0\n",
    "        self.Q2d = nn.Linear(embed_size,embed_size)\n",
    "        self.K2d = nn.Linear(embed_size,embed_size)\n",
    "        self.V2d = nn.Linear(embed_size,embed_size)\n",
    "        self.d2O = nn.Linear(embed_size,embed_size)\n",
    "        if test:\n",
    "            for name,parameter in self.Q2d.named_parameters():\n",
    "                parameter.data.fill_(.1)\n",
    "            for name,parameter in self.K2d.named_parameters():\n",
    "                parameter.data.fill_(.1)\n",
    "            for name,parameter in self.V2d.named_parameters():\n",
    "                parameter.data.fill_(.1)\n",
    "            for name,parameter in self.d2O.named_parameters():\n",
    "                parameter.data.fill_(.1)\n",
    "            dout_p = 0\n",
    "        self.dropout = nn.Dropout(dout_p)\n",
    "    def forward(self,Q,K,V,mask=None):\n",
    "        B , T , embed_size = Q.shape\n",
    "        Q = self.Q2d(Q)\n",
    "        K = self.K2d(K)\n",
    "        V = self.V2d(V)\n",
    "        Q = Q.reshape(B , T , self.heads, self.n_heads)\n",
    "        K = K.reshape(B , T , self.heads, self.n_heads)\n",
    "        V = V.reshape(B , T , self.heads, self.n_heads)\n",
    "        \n",
    "\n",
    "        attention = torch.einsum('bqhd,bkhd->bhqk',[Q,K])\n",
    "        attention = attention / np.sqrt(self.n_heads)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)#.unsqueeze(2)\n",
    "            attention = attention.masked_fill(mask==0,-float('inf'))\n",
    "        attention = torch.softmax(attention,dim=-1)\n",
    "\n",
    "        out = torch.einsum('bhqk,bkhd->bqhd',[attention,V])\n",
    "        out = self.dropout(out)\n",
    "        out = out.contiguous().view(B,T,embed_size)\n",
    "        out = self.d2O(out)\n",
    "        return out\n",
    "def attention(Q, K, V, mask=None, dropout=None, visual=False):\n",
    "    # Q, K, V are (B, *(H), seq_len, d_model//H = d_k)\n",
    "    # mask is     (B,    1,       1,               Ss)\n",
    "    d_k = Q.size(-1)\n",
    "    # (B, H, S, S)\n",
    "    QKt = Q.matmul(K.transpose(-1, -2))\n",
    "    sm_input = QKt / np.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        sm_input = sm_input.masked_fill(mask == 0, -float('inf'))\n",
    "\n",
    "    # try:\n",
    "    softmax = F.softmax(sm_input, dim=-1)\n",
    "    # except:\n",
    "    #     print('softmax failed: ' , sm_input)\n",
    "    #     raise ValueError(sm_input)\n",
    "\n",
    "\n",
    "    out = softmax.matmul(V)\n",
    "\n",
    "    if dropout is not None:\n",
    "        out = dropout(out)\n",
    "\n",
    "    # (B, *(H), seq_len, d_model//H = d_k)\n",
    "    if visual:\n",
    "        return out, softmax.detach()\n",
    "    else:\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiheadedAttention(nn.Module):\n",
    "    def __init__(self, d_model_Q, d_model_K, d_model_V, H, dout_p=0.0, d_model=None, d_out=None, test= False):\n",
    "        super(MultiheadedAttention, self).__init__()\n",
    "        self.d_model_Q = d_model_Q\n",
    "        self.d_model_K = d_model_K\n",
    "        self.d_model_V = d_model_V\n",
    "        self.H = H\n",
    "        self.d_model = d_model\n",
    "        self.dout_p = dout_p\n",
    "        self.d_out = d_out\n",
    "        if self.d_out is None:\n",
    "            self.d_out = self.d_model_Q\n",
    "\n",
    "        if self.d_model is None:\n",
    "            self.d_model = self.d_model_Q\n",
    "\n",
    "        self.d_k = self.d_model // H\n",
    "\n",
    "        self.linear_Q2d = nn.Linear(self.d_model_Q, self.d_model)\n",
    "        self.linear_K2d = nn.Linear(self.d_model_K, self.d_model)\n",
    "        self.linear_V2d = nn.Linear(self.d_model_V, self.d_model)\n",
    "        self.linear_d2Q = nn.Linear(self.d_model, self.d_out)\n",
    "\n",
    "        if test :\n",
    "            for name,parameter in self.linear_Q2d.named_parameters():\n",
    "                parameter.data.fill_(.1)\n",
    "            for name,parameter in self.linear_K2d.named_parameters():\n",
    "                parameter.data.fill_(.1)\n",
    "            for name,parameter in self.linear_V2d.named_parameters():\n",
    "                parameter.data.fill_(.1)\n",
    "            for name,parameter in self.linear_d2Q.named_parameters():\n",
    "                parameter.data.fill_(.1)\n",
    "            self.dout_p = 0\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dout_p)\n",
    "        self.visual = False\n",
    "\n",
    "        assert self.d_model % H == 0\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        ''' \n",
    "            Q, K, V: (B, Sq, Dq), (B, Sk, Dk), (B, Sv, Dv)\n",
    "            mask: (B, 1, Sk)\n",
    "            Sk = Sv, \n",
    "            Dk != self.d_k\n",
    "        '''\n",
    "        B, Sq, d_model_Q = Q.shape\n",
    "        # (B, Sm, D) <- (B, Sm, Dm)\n",
    "        Q = self.linear_Q2d(Q)\n",
    "        K = self.linear_K2d(K)\n",
    "        V = self.linear_V2d(V)\n",
    "\n",
    "        # (B, H, Sm, d_k) <- (B, Sm, D)\n",
    "        Q = Q.view(B, -1, self.H, self.d_k).transpose(-3, -2)  # (-4, -3*, -2*, -1)\n",
    "        K = K.view(B, -1, self.H, self.d_k).transpose(-3, -2)\n",
    "        V = V.view(B, -1, self.H, self.d_k).transpose(-3, -2)\n",
    "\n",
    "        if mask is not None:\n",
    "            # the same mask for all heads -> (B, 1, 1, Sm2)\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # (B, H, Sq, d_k) <- (B, H, Sq, d_k), (B, H, Sk, d_k), (B, H, Sv, d_k), Sk = Sv\n",
    "        if self.visual:\n",
    "            Q, self.attn_matrix = attention(Q, K, V, mask, self.dropout, self.visual)\n",
    "            self.attn_matrix = self.attn_matrix.mean(-3)\n",
    "        else:\n",
    "            Q = attention(Q, K, V, mask, self.dropout)\n",
    "        # (B, Sq, D) <- (B, H, Sq, d_k)\n",
    "        Q = Q.transpose(-3, -2).contiguous().view(B, Sq, self.d_model)\n",
    "        # (B, Sq, Dq)\n",
    "        Q = self.linear_d2Q(Q)\n",
    "\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingLayer(nn.Module):\n",
    "    def __init__(self,embed_size,dout_p,test=False):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.residualNetwork_1 = ResidualNetwork(embed_size,dout_p=.1,test=test)\n",
    "        #self.residualNetwork_2 = ResidualNetwork(embed_size,dout_p=.1,test=test)\n",
    "\n",
    "        if test:\n",
    "            dout_p = 0\n",
    "\n",
    "        print(embed_size,dout_p,test)\n",
    "        self.attention = Attention(embed_size,dout_p,test=test)\n",
    "        self.feedForward = nn.Sequential(\n",
    "            nn.Linear(embed_size,4 * embed_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dout_p),\n",
    "            nn.Linear(4 * embed_size,embed_size)\n",
    "        )\n",
    "\n",
    "        if test:\n",
    "            for name,parameter in self.feedForward.named_parameters():\n",
    "                parameter.data.fill_(.1)\n",
    "\n",
    "    def forward(self,x,video_mask):\n",
    "        return x\n",
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, dout_p, H=8, d_ff=None, d_hidden=None , test = False):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_att = MultiheadedAttention(256, 256, 256, 8, d_model=256, test=test)\n",
    "        \n",
    "    def forward(self, x, src_mask=None):\n",
    "        '''\n",
    "        in:\n",
    "            x: (B, S, d_model), src_mask: (B, 1, S)\n",
    "        out:\n",
    "            (B, S, d_model)\n",
    "        '''\n",
    "        # sublayer should be a function which inputs x and outputs transformation\n",
    "        # thus, lambda is used instead of just `self.self_att(x, x, x)` which outputs \n",
    "        # the output of the self attention\n",
    "        sublayer0 = lambda x: self.self_att(x, x, x, src_mask)\n",
    "        sublayer1 = self.feed_forward\n",
    "        \n",
    "        #x = self.res_layer0(x, sublayer0)\n",
    "        #x = self.res_layer1(x, sublayer1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 8 256 True\n",
      "256 0 True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.carl_transformer.transformer import EncoderLayer\n",
    "from model.transformer.encoder.encodingLayer.encodingLayer import EncodingLayer\n",
    "\n",
    "carl_en = EncoderLayer(256,0,test=True)\n",
    "my_en = EncodingLayer(256,0,test=True)\n",
    "\n",
    "#carl_en.self_att = MultiheadedAttention(256,256,256,8,d_model=256,test=True)\n",
    "#my_en.attention=Attention(256,0,test=True)\n",
    "\n",
    "en_data = torch.rand(2,10,256)\n",
    "torch.equal(carl_en(en_data),my_en(en_data,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self,test=False):\n",
    "        super().__init__()\n",
    "        projection_hidden_size = cfg.MODEL.PROJECTION_SIZE\n",
    "        self.embedding_size = cfg.MODEL.EMBEDDER_MODEL.EMBEDDING_SIZE\n",
    "        assert projection_hidden_size==128\n",
    "        assert self.embedding_size==128\n",
    "        self.net = nn.Sequential(nn.Linear(self.embedding_size, projection_hidden_size),\n",
    "                                nn.BatchNorm1d(projection_hidden_size),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(projection_hidden_size, self.embedding_size))\n",
    "        for name,paramter in self.net.named_parameters():\n",
    "                paramter.data.fill_(.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, l, c = x.shape\n",
    "        x = x.view(-1,c)\n",
    "        x = self.net(x)\n",
    "        return x.view(b, l, c)\n",
    "my_projection = nn.Sequential(\n",
    "                nn.Linear(128,128),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(128,128)\n",
    "            )\n",
    "carl_projection = MLPHead(test=True)\n",
    "\n",
    "for name,parameter in my_projection.named_parameters():\n",
    "    parameter.data.fill_(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_data = torch.rand(2,10,128)\n",
    "my_pro_data = pro_data.view(-1,128)\n",
    "my_pro_data = my_projection(my_pro_data)\n",
    "my_pro_data = my_pro_data.view(2,10,128)\n",
    "carl_pro_data= carl_projection(pro_data)\n",
    "torch.equal(my_pro_data,carl_pro_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carl",
   "language": "python",
   "name": "carl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 21:20:05.826132: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "\n",
    "def regression_loss(logits, labels, num_steps, steps, seq_lens, loss_type,\n",
    "                    normalize_indices, variance_lambda, huber_delta):\n",
    "  \"\"\"Loss function based on regressing to the correct indices.\n",
    "  In the paper, this is called Cycle-back Regression. There are 3 variants\n",
    "  of this loss:\n",
    "  i) regression_mse: MSE of the predicted indices and ground truth indices.\n",
    "  ii) regression_mse_var: MSE of the predicted indices that takes into account\n",
    "  the variance of the similarities. This is important when the rate at which\n",
    "  sequences go through different phases changes a lot. The variance scaling\n",
    "  allows dynamic weighting of the MSE loss based on the similarities.\n",
    "  iii) regression_huber: Huber loss between the predicted indices and ground\n",
    "  truth indices.\n",
    "  Args:\n",
    "    logits: Tensor, Pre-softmax similarity scores after cycling back to the\n",
    "      starting sequence.\n",
    "    labels: Tensor, One hot labels containing the ground truth. The index where\n",
    "      the cycle started is 1.\n",
    "    num_steps: Integer, Number of steps in the sequence embeddings.\n",
    "    steps: Tensor, step indices/frame indices of the embeddings of the shape\n",
    "      [N, T] where N is the batch size, T is the number of the timesteps.\n",
    "    seq_lens: Tensor, Lengths of the sequences from which the sampling was done.\n",
    "      This can provide additional temporal information to the alignment loss.\n",
    "    loss_type: String, This specifies the kind of regression loss function.\n",
    "      Currently supported loss functions: regression_mse, regression_mse_var,\n",
    "      regression_huber.\n",
    "    normalize_indices: Boolean, If True, normalizes indices by sequence lengths.\n",
    "      Useful for ensuring numerical instabilities don't arise as sequence\n",
    "      indices can be large numbers.\n",
    "    variance_lambda: Float, Weight of the variance of the similarity\n",
    "      predictions while cycling back. If this is high then the low variance\n",
    "      similarities are preferred by the loss while making this term low results\n",
    "      in high variance of the similarities (more uniform/random matching).\n",
    "    huber_delta: float, Huber delta described in tf.keras.losses.huber_loss.\n",
    "  Returns:\n",
    "     loss: Tensor, A scalar loss calculated using a variant of regression.\n",
    "  \"\"\"\n",
    "  # Just to be safe, we stop gradients from labels as we are generating labels.\n",
    "  labels = tf.stop_gradient(labels)\n",
    "  steps = tf.stop_gradient(steps)\n",
    "\n",
    "  # tf.print(\"logits\" , logits ,output_stream=sys.stderr)\n",
    "  # tf.print(\"labels\" , labels ,output_stream=sys.stderr)\n",
    "\n",
    "  if normalize_indices:\n",
    "    float_seq_lens = tf.cast(seq_lens, tf.float32)\n",
    "    tile_seq_lens = tf.tile(\n",
    "        tf.expand_dims(float_seq_lens, axis=1), [1, num_steps])\n",
    "    steps = tf.cast(steps, tf.float32) / tile_seq_lens\n",
    "  else:\n",
    "    steps = tf.cast(steps, tf.float32)\n",
    "\n",
    "  beta = tf.nn.softmax(logits)\n",
    "  # print('steps.shape: ', steps.shape)\n",
    "  # print('labels.shape: ', labels.shape)\n",
    "  true_time = tf.reduce_sum(steps * labels, axis=1)\n",
    "  pred_time = tf.reduce_sum(steps * beta, axis=1)\n",
    "\n",
    "  if loss_type in ['regression_mse', 'regression_mse_var']:\n",
    "    if 'var' in loss_type:\n",
    "      # Variance aware regression.\n",
    "      pred_time_tiled = tf.tile(tf.expand_dims(pred_time, axis=1),\n",
    "                                [1, num_steps])\n",
    "      \n",
    "      pred_time_variance = tf.reduce_sum(\n",
    "          tf.square(steps - pred_time_tiled) * beta, axis=1)\n",
    "\n",
    "      # Using log of variance as it is numerically stabler.\n",
    "      pred_time_log_var = tf.math.log(pred_time_variance)\n",
    "      squared_error = tf.square(true_time - pred_time)\n",
    "      \n",
    "      distance = tf.math.exp(-pred_time_log_var) * squared_error \\\n",
    "                            + variance_lambda * pred_time_log_var\n",
    "      return tf.reduce_mean(distance)\n",
    "\n",
    "    else:\n",
    "      return tf.reduce_mean(\n",
    "          tf.keras.losses.mean_squared_error(y_true=true_time,\n",
    "                                             y_pred=pred_time))\n",
    "  elif loss_type == 'regression_huber':\n",
    "    return tf.reduce_mean(tf.keras.losses.huber_loss(\n",
    "        y_true=true_time, y_pred=pred_time,\n",
    "        delta=huber_delta))\n",
    "  else:\n",
    "    raise ValueError('Unsupported regression loss %s. Supported losses are: '\n",
    "                     'regression_mse, regresstion_mse_var and regression_huber.'\n",
    "                     % loss_type)\n",
    "    \n",
    "    \n",
    "def pairwise_l2_distance(embs1, embs2):\n",
    "  \"\"\"Computes pairwise distances between all rows of embs1 and embs2.\"\"\"\n",
    "  norm1 = tf.reduce_sum(tf.square(embs1), 1)\n",
    "  norm1 = tf.reshape(norm1, [-1, 1])\n",
    "  norm2 = tf.reduce_sum(tf.square(embs2), 1)\n",
    "  norm2 = tf.reshape(norm2, [1, -1])\n",
    "\n",
    "  # Max to ensure matmul doesn't produce anything negative due to floating\n",
    "  # point approximations.\n",
    "  dist = tf.maximum(\n",
    "      norm1 + norm2 - 2.0 * tf.matmul(embs1, embs2, False, True), 0.0)\n",
    "\n",
    "  return dist\n",
    "\n",
    "def get_scaled_similarity(embs1, embs2, similarity_type, temperature):\n",
    "  \"\"\"Returns similarity between each all rows of embs1 and all rows of embs2.\n",
    "  The similarity is scaled by the number of channels/embedding size and\n",
    "  temperature.\n",
    "  Args:\n",
    "    embs1: Tensor, Embeddings of the shape [M, D] where M is the number of\n",
    "      embeddings and D is the embedding size.\n",
    "    embs2: Tensor, Embeddings of the shape [N, D] where N is the number of\n",
    "      embeddings and D is the embedding size.\n",
    "    similarity_type: String, Either one of 'l2' or 'cosine'.\n",
    "    temperature: Float, Temperature used in scaling logits before softmax.\n",
    "  Returns:\n",
    "    similarity: Tensor, [M, N] tensor denoting similarity between embs1 and\n",
    "      embs2.\n",
    "  \"\"\"\n",
    "  channels = tf.cast(tf.shape(embs1)[1], tf.float32)\n",
    "  # Go for embs1 to embs2.\n",
    "  if similarity_type == 'cosine':\n",
    "    similarity = tf.matmul(embs1, embs2, transpose_b=True)\n",
    "  elif similarity_type == 'l2':\n",
    "    similarity = -1.0 * pairwise_l2_distance(embs1, embs2)\n",
    "  else:\n",
    "    raise ValueError('similarity_type can either be l2 or cosine.')\n",
    "\n",
    "  # Scale the distance  by number of channels. This normalization helps with\n",
    "  # optimization.\n",
    "  similarity /= channels\n",
    "  # Scale the distance by a temperature that helps with how soft/hard the\n",
    "  # alignment should be.\n",
    "  similarity /= temperature\n",
    "  \n",
    "  return similarity\n",
    "\n",
    "def align_pair_of_sequences(embs1,\n",
    "                            embs2,\n",
    "                            similarity_type,\n",
    "                            temperature):\n",
    "  \"\"\"Align a given pair embedding sequences.\n",
    "  Args:\n",
    "    embs1: Tensor, Embeddings of the shape [M, D] where M is the number of\n",
    "      embeddings and D is the embedding size.\n",
    "    embs2: Tensor, Embeddings of the shape [N, D] where N is the number of\n",
    "      embeddings and D is the embedding size.\n",
    "    similarity_type: String, Either one of 'l2' or 'cosine'.\n",
    "    temperature: Float, Temperature used in scaling logits before softmax.\n",
    "  Returns:\n",
    "     logits: Tensor, Pre-softmax similarity scores after cycling back to the\n",
    "      starting sequence.\n",
    "    labels: Tensor, One hot labels containing the ground truth. The index where\n",
    "      the cycle started is 1.\n",
    "  \"\"\"\n",
    "  # max_num_steps is embs1's frame number\n",
    "  max_num_steps = tf.shape(embs1)[0]\n",
    "\n",
    "  # Find distances between embs1 and embs2.\n",
    "  sim_12 = get_scaled_similarity(embs1, embs2, similarity_type, temperature)\n",
    "  \n",
    "  # Softmax the distance.\n",
    "  softmaxed_sim_12 = tf.nn.softmax(sim_12, axis=1)\n",
    "  # softmaxed_sim_12 is alpha in TCC paper\n",
    "  # Calculate soft-nearest neighbors.\n",
    "  nn_embs = tf.matmul(softmaxed_sim_12, embs2)\n",
    "  # Find distances between nn_embs and embs1.\n",
    "  sim_21 = get_scaled_similarity(nn_embs, embs1, similarity_type, temperature)\n",
    "\n",
    "  # tf.print(\"emb1\" , embs1 ,output_stream=sys.stderr)\n",
    "  # tf.print(\"emb2\" , embs2 ,output_stream=sys.stderr)\n",
    "  # tf.print(\"sim_12\" , sim_12 ,output_stream=sys.stderr)\n",
    "  # tf.print(\"softmaxed_sim_12\" , softmaxed_sim_12 ,output_stream=sys.stderr)\n",
    "  # tf.print(\"nn_embs\" , nn_embs ,output_stream=sys.stderr)\n",
    "  # tf.print(\"sim_21\" , sim_21 ,output_stream=sys.stderr)\n",
    "\n",
    "\n",
    "\n",
    "  logits = sim_21\n",
    "  labels = tf.one_hot(tf.range(max_num_steps), max_num_steps)\n",
    "\n",
    "  return logits, labels\n",
    "\n",
    "\n",
    "def compute_deterministic_alignment_loss(embs,\n",
    "                                         steps,\n",
    "                                         seq_lens,\n",
    "                                         num_steps,\n",
    "                                         batch_size,\n",
    "                                         loss_type,\n",
    "                                         similarity_type,\n",
    "                                         temperature,\n",
    "                                         label_smoothing,\n",
    "                                         variance_lambda,\n",
    "                                         huber_delta,\n",
    "                                         normalize_indices):\n",
    "  \"\"\"Compute cycle-consistency loss for all steps in each sequence.\n",
    "  This aligns each pair of videos in the batch except with itself.\n",
    "  When aligning it also matters which video is the starting video. So for N\n",
    "  videos in the batch, we have N * (N-1) alignments happening.\n",
    "  For example, a batch of size 3 has 6 pairs of sequence alignments.\n",
    "  Args:\n",
    "    embs: Tensor, sequential embeddings of the shape [N, T, D] where N is the\n",
    "      batch size, T is the number of timesteps in the sequence, D is the size\n",
    "      of the embeddings.\n",
    "    steps: Tensor, step indices/frame indices of the embeddings of the shape\n",
    "      [N, T] where N is the batch size, T is the number of the timesteps.\n",
    "    seq_lens: Tensor, Lengths of the sequences from which the sampling was\n",
    "    done. This can provide additional information to the alignment loss.\n",
    "    num_steps: Integer/Tensor, Number of timesteps in the embeddings.\n",
    "    batch_size: Integer, Size of the batch.\n",
    "    loss_type: String, This specifies the kind of loss function to use.\n",
    "      Currently supported loss functions: 'classification', 'regression_mse',\n",
    "      'regression_mse_var', 'regression_huber'.\n",
    "    similarity_type: String, Currently supported similarity metrics: 'l2' ,\n",
    "      'cosine' .\n",
    "    temperature: Float, temperature scaling used to scale the similarity\n",
    "      distributions calculated using the softmax function.\n",
    "    label_smoothing: Float, Label smoothing argument used in\n",
    "      tf.keras.losses.categorical_crossentropy function and described in this\n",
    "      paper https://arxiv.org/pdf/1701.06548.pdf.\n",
    "    variance_lambda: Float, Weight of the variance of the similarity\n",
    "      predictions while cycling back. If this is high then the low variance\n",
    "      similarities are preferred by the loss while making this term low\n",
    "      results in high variance of the similarities (more uniform/random\n",
    "      matching).\n",
    "    huber_delta: float, Huber delta described in tf.keras.losses.huber_loss.\n",
    "    normalize_indices: Boolean, If True, normalizes indices by sequence\n",
    "      lengths. Useful for ensuring numerical instabilities doesn't arise as\n",
    "      sequence indices can be large numbers.\n",
    "  Returns:\n",
    "    loss: Tensor, Scalar loss tensor that imposes the chosen variant of the\n",
    "        cycle-consistency loss.\n",
    "  \"\"\"\n",
    "\n",
    "  # print('b4, steps shape:' , steps.shape)\n",
    "  labels_list = []\n",
    "  logits_list = []\n",
    "  steps_list = []\n",
    "  seq_lens_list = []\n",
    "\n",
    "  for i in range(batch_size):\n",
    "    for j in range(batch_size):\n",
    "      # We do not align the sequence with itself.\n",
    "      if i != j:\n",
    "        # print(embs.shape , batch_size)\n",
    "        logits, labels = align_pair_of_sequences(embs[i],\n",
    "                                                 embs[j],\n",
    "                                                 similarity_type,\n",
    "                                                 temperature)\n",
    "        logits_list.append(logits)\n",
    "        labels_list.append(labels)\n",
    "        steps_list.append(tf.tile(steps[i:i+1], [num_steps, 1]))\n",
    "        seq_lens_list.append(tf.tile(seq_lens[i:i+1], [(num_steps)]))\n",
    "\n",
    "\n",
    "  logits = tf.concat(logits_list, axis=0)\n",
    "  labels = tf.concat(labels_list, axis=0)\n",
    "  steps = tf.concat(steps_list, axis=0)\n",
    "  seq_lens = tf.concat(seq_lens_list, axis=0)\n",
    "  # print('logit.shape',logits.shape)\n",
    "  # print('label.shape',labels.shape)\n",
    "  # print('steps.shape',steps.shape)\n",
    "  # print('seq_lens.shape',seq_lens.shape)\n",
    "\n",
    "\n",
    "  if True:\n",
    "    loss = regression_loss(logits, labels, num_steps, steps, seq_lens,\n",
    "                           loss_type, normalize_indices, variance_lambda,\n",
    "                           huber_delta)\n",
    "    \n",
    "  else:\n",
    "    raise ValueError('Unidentified loss_type %s. Currently supported loss '\n",
    "                     'types are: regression_mse, regression_huber, '\n",
    "                     'classification.' % loss_type)\n",
    "  \n",
    "  return loss,logits,labels,steps,seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "class TCC():\n",
    "    def __init__(self,cfg):\n",
    "        self.label_smoothing = cfg.TCC.LABEL_SMOOTHING\n",
    "        self.temperature = cfg.TCC.SOFTMAX_TEMPERATURE\n",
    "        self.lambda_ = cfg.TCC.VARIANCE_LAMBDA\n",
    "        pass\n",
    "    def compute_cycle_loss(self,emb_x,emb_y):\n",
    "        \"\"\"\n",
    "        1. Find the distance for emb_x in emb_y (using l2)\n",
    "        2. softmax the distance then multiply by emb_y\n",
    "        3. use 2. to find the distance for emb_x as logits\n",
    "        4. construct smoothed labels \n",
    "        \"\"\"\n",
    "        num_steps, D = emb_x.shape\n",
    "\n",
    "        distance = -1 * torch.cdist(emb_x,emb_y,p=2).pow(2)\n",
    "        distance = distance / D / self.temperature\n",
    "\n",
    "        sftmax = torch.softmax(distance,dim=-1)\n",
    "        emb_y = torch.matmul(sftmax,emb_y)\n",
    "        logits = -1 * torch.cdist(emb_y,emb_x,p=2).pow(2) / D / self.temperature\n",
    "\n",
    "        labels = torch.diag(torch.ones(num_steps)).type_as(logits)\n",
    "        ## label smoothing\n",
    "        # labels = (1-num_steps*self.label_smoothing/(num_steps-1))*labels + \\\n",
    "        #                 self.label_smoothing/(num_steps-1)*torch.ones_like(labels)\n",
    "        return logits,labels\n",
    "    \n",
    "    def regression_loss(self,logits,labels,steps,seq_lens):\n",
    "        \"\"\"\n",
    "        1. Normalize the steps by seq_lens (to mitigate the influence of long seq_lens)\n",
    "        2. softmax logits to obtain beta\n",
    "        3. calculate i by multiplying labels and steps\n",
    "        4. calculate mean by multiplying beta and steps\n",
    "        5. calculate variance by multiplying beta and (i - mean)^2\n",
    "        6. calculate Lcbr\n",
    "        \"\"\"\n",
    "        # steps = steps / seq_lens.unsqueeze(1)\n",
    "        beta = torch.softmax(logits,dim=-1)\n",
    "        i = torch.sum(steps*labels,dim=-1)\n",
    "        mean = torch.sum(steps*beta,dim=-1)\n",
    "        variance = torch.sum(torch.square(steps-mean.unsqueeze(1)) * beta ,dim=-1)\n",
    "        log_variance = torch.log(variance)\n",
    "        Lcbr = torch.mean(torch.square(i-mean) / variance + self.lambda_ * log_variance)\n",
    "        return Lcbr\n",
    "        \n",
    "    def compute_loss(self,embs,steps,seq_lens,batch_size=2):\n",
    "        \"\"\"\n",
    "        TCC computes loss based on the following equation:\n",
    "            1. find the soft nearest neighbor for u in v, this is acheived by softmaxing\n",
    "            2. compute betak, the formula is given as exp(-d(v,uk)) / sum(exp(-d(v,uj)))\n",
    "        \"\"\"\n",
    "        steps = steps.to(embs.device)\n",
    "        seq_lens = seq_lens.to(embs.device)\n",
    "\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "        steps_list = []\n",
    "        seq_lens_list = []\n",
    "\n",
    "        B , T , D = embs.shape\n",
    "        for i in range((batch_size)):\n",
    "            for j in range((batch_size)):\n",
    "                if i ==j:\n",
    "                    continue\n",
    "                logits,labels = self.compute_cycle_loss(embs[i],embs[j])\n",
    "                logits_list.append(logits)\n",
    "                labels_list.append(labels)\n",
    "                steps_list.append(steps[i].unsqueeze(0).expand(T,T))\n",
    "                seq_lens_list.append(seq_lens[i].view(1,).expand(T))\n",
    "        logits_list = torch.cat(logits_list,dim=0)\n",
    "        labels_list = torch.cat(labels_list,dim=0)\n",
    "        steps_list = torch.cat(steps_list,dim=0)\n",
    "        seq_lens_list = torch.cat(seq_lens_list,dim=0)\n",
    "\n",
    "        loss = self.regression_loss(logits_list,labels_list,steps_list,seq_lens_list)\n",
    "        return loss,logits_list,labels_list,steps_list,seq_lens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import torch\n",
    "\n",
    "class TCC_CARL(object):\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.loss_type=cfg.TCC.LOSS_TYPE\n",
    "        self.similarity_type=cfg.TCC.SIMILARITY_TYPE\n",
    "        self.cycle_length=cfg.TCC.CYCLE_LENGTH\n",
    "        self.temperature=cfg.TCC.SOFTMAX_TEMPERATURE\n",
    "        self.label_smoothing=cfg.TCC.LABEL_SMOOTHING\n",
    "        self.variance_lambda=cfg.TCC.VARIANCE_LAMBDA\n",
    "        self.huber_delta=cfg.TCC.HUBER_DELTA\n",
    "        self.normalize_indices=cfg.TCC.NORMALIZE_INDICES\n",
    "\n",
    "    def compute_loss(self, embs,  chosen_steps,seq_lens):\n",
    "        \"\"\"One pass through the model.\n",
    "\n",
    "        Args:\n",
    "        videos: Tensor, batches of tensors from many videos.\n",
    "        training: Boolean, if True model is run in training mode.\n",
    "\n",
    "        Returns:\n",
    "        loss: Tensor, Float tensor containing loss\n",
    "        \"\"\"\n",
    "        # num_frames = self.cfg.TRAIN.NUM_FRAMES\n",
    "\n",
    "        # if self.cfg.SSL:\n",
    "        #     batch_size, num_views, num_steps, c, h, w = videos.shape\n",
    "        #     videos = videos.view(-1, num_steps, c, h, w)\n",
    "        #     chosen_steps = chosen_steps.view(-1, num_frames)\n",
    "        #     seq_lens = seq_lens.view(batch_size, num_views).view(-1)\n",
    "        # else:\n",
    "        #     batch_size, num_steps, c, h, w = videos.shape\n",
    "        # if video_masks is not None:\n",
    "        #     video_masks = video_masks.view(-1, 1, num_steps)\n",
    "        # embs = model(videos, num_frames, video_masks=video_masks)\n",
    "        loss = self.compute_deterministic_alignment_loss(embs, seq_lens, chosen_steps)\n",
    "        return loss\n",
    "\n",
    "    def compute_deterministic_alignment_loss(self, embs, seq_lens, steps):\n",
    "\n",
    "        labels_list = []\n",
    "        logits_list = []\n",
    "        steps_list = []\n",
    "        seq_lens_list = []\n",
    "\n",
    "        batch_size, num_frames, channels = embs.shape\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(batch_size):\n",
    "                # We do not align the sequence with itself.\n",
    "                if i == j:\n",
    "                    continue\n",
    "                logits, labels = self.align_pair_of_sequences(embs[i], embs[j])\n",
    "                logits_list.append(logits)\n",
    "                labels_list.append(labels)\n",
    "                steps_list.append(steps[i].unsqueeze(0).expand(num_frames, num_frames))\n",
    "                seq_lens_list.append(seq_lens[i].view(1,).expand(num_frames))\n",
    "                \n",
    "        logits = torch.cat(logits_list,0)\n",
    "        labels = torch.cat(labels_list, 0)\n",
    "        steps = torch.cat(steps_list, 0)\n",
    "        seq_lens = torch.cat(seq_lens_list, 0)\n",
    "\n",
    "        if self.loss_type == 'classification':\n",
    "            loss = {\"loss\": torch.nn.KLDivLoss(reduction='mean')(logits, labels)}\n",
    "        elif 'regression' in self.loss_type:\n",
    "            loss = self.regression_loss(logits, labels, steps, seq_lens)\n",
    "\n",
    "        return loss,logits,labels,steps,seq_lens\n",
    "\n",
    "    def align_pair_of_sequences(self, embs1, embs2):\n",
    "        \"\"\"Align a given pair embedding sequences.\n",
    "\n",
    "        Args:\n",
    "            embs1: Tensor, Embeddings of the shape [M, D] where M is the number of\n",
    "            embeddings and D is the embedding size.\n",
    "            embs2: Tensor, Embeddings of the shape [N, D] where N is the number of\n",
    "            embeddings and D is the embedding size.\n",
    "        Returns:\n",
    "            logits: Tensor, Pre-softmax similarity scores after cycling back to the\n",
    "            starting sequence.\n",
    "            labels: Tensor, One hot labels containing the ground truth. The index where\n",
    "            the cycle started is 1.\n",
    "        \"\"\"\n",
    "        num_steps, channels = embs1.shape\n",
    "\n",
    "        sim_12 = self.get_scaled_similarity(embs1, embs2)\n",
    "        # Softmax the distance.\n",
    "        softmaxed_sim_12 = torch.softmax(sim_12, dim=-1)\n",
    "\n",
    "        # Calculate soft-nearest neighbors.\n",
    "        nn_embs = torch.matmul(softmaxed_sim_12, embs2)\n",
    "\n",
    "        # Find distances between nn_embs and embs1.\n",
    "        sim_21 = self.get_scaled_similarity(nn_embs, embs1)\n",
    "\n",
    "        logits = sim_21\n",
    "        labels = torch.diag(torch.ones(num_steps)).type_as(logits)\n",
    "        if self.label_smoothing:\n",
    "            labels = (1-num_steps*self.label_smoothing/(num_steps-1))*labels + \\\n",
    "                        self.label_smoothing/(num_steps-1)*torch.ones_like(labels)\n",
    "\n",
    "        return logits, labels\n",
    "\n",
    "    def get_scaled_similarity(self, embs1, embs2):\n",
    "        num_steps, channels = embs1.shape\n",
    "        # Find distances between embs1 and embs2.\n",
    "        if self.similarity_type == 'cosine':\n",
    "            sim_12 = torch.matmul(embs1, embs2.transpose(0,1))\n",
    "        elif self.similarity_type == 'l2':\n",
    "            norm1 = torch.square(embs1).sum(1).view(-1,1)\n",
    "            norm2 = torch.square(embs2).sum(1).view(1,-1)\n",
    "            sim_12 = - (norm1 + norm2 - 2*torch.matmul(embs1, embs2.transpose(0,1)))\n",
    "        else:\n",
    "            raise ValueError('Unsupported similarity type %s.' % self.similarity_type)\n",
    "        return sim_12 / channels / self.temperature\n",
    "\n",
    "    def regression_loss(self, logits, labels, steps, seq_lens):\n",
    "        \"\"\"Loss function based on regressing to the correct indices.\n",
    "\n",
    "        In the paper, this is called Cycle-back Regression. There are 3 variants\n",
    "        of this loss:\n",
    "        i) regression_mse: MSE of the predicted indices and ground truth indices.\n",
    "        ii) regression_mse_var: MSE of the predicted indices that takes into account\n",
    "        the variance of the similarities. This is important when the rate at which\n",
    "        sequences go through different phases changes a lot. The variance scaling\n",
    "        allows dynamic weighting of the MSE loss based on the similarities.\n",
    "        iii) regression_huber: Huber loss between the predicted indices and ground\n",
    "        truth indices.\n",
    "\n",
    "\n",
    "        Args:\n",
    "            logits: Tensor, Pre-softmax similarity scores after cycling back to the\n",
    "            starting sequence.\n",
    "            labels: Tensor, One hot labels containing the ground truth. The index where\n",
    "            the cycle started is 1.\n",
    "            num_steps: Integer, Number of steps in the sequence embeddings.\n",
    "            steps: Tensor, step indices/frame indices of the embeddings of the shape\n",
    "            [N, T] where N is the batch size, T is the number of the timesteps.\n",
    "            seq_lens: Tensor, Lengths of the sequences from which the sampling was done.\n",
    "            This can provide additional temporal information to the alignment loss.\n",
    "\n",
    "            loss_type: String, This specifies the kind of regression loss function.\n",
    "            Currently supported loss functions: regression_mse, regression_mse_var,\n",
    "            regression_huber.\n",
    "            normalize_indices: Boolean, If True, normalizes indices by sequence lengths.\n",
    "            Useful for ensuring numerical instabilities don't arise as sequence\n",
    "            indices can be large numbers.\n",
    "            variance_lambda: Float, Weight of the variance of the similarity\n",
    "            predictions while cycling back. If this is high then the low variance\n",
    "            similarities are preferred by the loss while making this term low results\n",
    "            in high variance of the similarities (more uniform/random matching).\n",
    "            huber_delta: float, Huber delta described in tf.keras.losses.huber_loss.\n",
    "\n",
    "        Returns:\n",
    "            loss: Tensor, A scalar loss calculated using a variant of regression.\n",
    "        \"\"\"\n",
    "        steps = steps.type_as(logits)\n",
    "        if self.normalize_indices:\n",
    "            seq_lens = seq_lens.type_as(logits)\n",
    "            # print(\"steps\", steps.shape)\n",
    "            # print(\"seq_lens\", seq_lens.shape)\n",
    "            steps = steps / seq_lens.unsqueeze(1)\n",
    "\n",
    "        beta = torch.softmax(logits, dim=-1)\n",
    "        true_time = torch.sum(steps * labels, dim=-1)\n",
    "        pred_time = torch.sum( beta * steps, dim=-1)\n",
    "\n",
    "        if self.loss_type in ['regression_mse', 'regression_mse_var']:\n",
    "            if 'var' in self.loss_type:\n",
    "                # Variance aware regression.\n",
    "                pred_time_variance = torch.sum(torch.square(steps - pred_time.unsqueeze(-1)) * beta, dim=-1)\n",
    "                assert torch.min(pred_time_variance) > 0\n",
    "                # Using log of variance as it is numerically stabler.\n",
    "                pred_time_log_var = torch.log(pred_time_variance)\n",
    "                squared_error = torch.square(true_time - pred_time)\n",
    "                loss = torch.mean(torch.exp(-pred_time_log_var) * squared_error\n",
    "                                        + self.variance_lambda * pred_time_log_var)\n",
    "                # return {\"loss\": loss, \"squared_error\": torch.mean(squared_error), \n",
    "                #                     \"pred_time_log_var\": torch.mean(pred_time_log_var)}\n",
    "                return loss\n",
    "            else:\n",
    "                return {\"loss\": torch.nn.MSELoss()(pred_time, true_time)}\n",
    "        elif self.loss_type == 'regression_huber':\n",
    "            return {\"loss\": torch.nn.SmoothL1Loss()(pred_time, true_time)}\n",
    "        else:\n",
    "            raise ValueError('Unsupported regression loss %s. Supported losses are: '\n",
    "                            'regression_mse, regresstion_mse_var and regression_huber.'\n",
    "                            % self.loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 21:20:12.829104: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-04 21:20:16.925523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4711 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:3f:00.0, compute capability: 8.6\n",
      "2023-10-04 21:20:16.927696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22304 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:40:00.0, compute capability: 8.6\n",
      "2023-10-04 21:20:16.929004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 5511 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:86:00.0, compute capability: 8.6\n",
      "2023-10-04 21:20:16.930429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22838 MB memory:  -> device: 3, name: NVIDIA TITAN RTX, pci bus id: 0000:1c:00.0, compute capability: 7.5\n",
      "2023-10-04 21:20:16.931803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 22838 MB memory:  -> device: 4, name: NVIDIA TITAN RTX, pci bus id: 0000:1d:00.0, compute capability: 7.5\n",
      "2023-10-04 21:20:16.933230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 6577 MB memory:  -> device: 5, name: NVIDIA TITAN RTX, pci bus id: 0000:1e:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Create a sample input tensor\n",
    "N, T, D = 2, 50, 128\n",
    "embs = tf.random.normal((N, T, D))\n",
    "steps = tf.sort(tf.random.uniform((N, T), maxval=T, dtype=tf.int32))\n",
    "seq_lens = tf.random.uniform((N,), maxval=T, dtype=tf.int32)\n",
    "\n",
    "num_embs = embs.numpy()\n",
    "num_steps = steps.numpy()\n",
    "num_seq_lens = seq_lens.numpy()\n",
    "\n",
    "torch_embs = torch.from_numpy(num_embs)\n",
    "torch_steps = torch.from_numpy(num_steps)\n",
    "torch_seq_lens = torch.from_numpy(num_seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 21:20:18.139392: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.5241928>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_steps = T\n",
    "batch_size = N\n",
    "loss_type = 'regression_mse_var'\n",
    "similarity_type = 'l2'\n",
    "temperature = 1.0\n",
    "label_smoothing = None\n",
    "variance_lambda = 0.1\n",
    "huber_delta = 1.0\n",
    "normalize_indices = False\n",
    "\n",
    "# Compute the loss for the sample input tensor\n",
    "tf_loss,tf_logits_list,tf_labels_list,tf_steps_list,tf_seq_lens_list = compute_deterministic_alignment_loss(embs, steps, seq_lens,\n",
    "                                             num_steps, batch_size,\n",
    "                                             loss_type,\n",
    "                                             similarity_type,\n",
    "                                             temperature,\n",
    "                                             label_smoothing,\n",
    "                                             variance_lambda,\n",
    "                                             huber_delta,normalize_indices)\n",
    "\n",
    "# Print the loss\n",
    "tf_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5242)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a TCC object with default configuration\n",
    "cfg = type('', (), {})()\n",
    "cfg.TCC = type('', (), {})()\n",
    "cfg.TCC.LABEL_SMOOTHING = None\n",
    "cfg.TCC.SOFTMAX_TEMPERATURE = 1.0\n",
    "cfg.TCC.VARIANCE_LAMBDA = 0.1\n",
    "tcc = TCC(cfg)\n",
    "\n",
    "# Compute the loss for the sample input tensor\n",
    "torch_loss,torch_logits_list,torch_labels_list,torch_steps_list,torch_seq_lens_list = tcc.compute_loss(torch_embs, torch_steps, torch_seq_lens)\n",
    "\n",
    "torch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5242)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from easydict import EasyDict as edict\n",
    "# Create a TCC object with default configuration\n",
    "cfg = {'TCC': {'LOSS_TYPE': 'regression_mse_var', 'SIMILARITY_TYPE': 'l2', 'CYCLE_LENGTH': 10, 'SOFTMAX_TEMPERATURE': 1, 'LABEL_SMOOTHING': None, 'VARIANCE_LAMBDA': 0.1, 'HUBER_DELTA': 0.1, 'NORMALIZE_INDICES': False}, 'TRAIN': {'NUM_FRAMES': 16}, 'SSL': False}\n",
    "cfg = edict(cfg)\n",
    "carl_tcc = TCC_CARL(cfg)\n",
    "\n",
    "carl_loss,carl_logits_list,carl_labels_list,carl_steps_list,carl_seq_lens_list = carl_tcc.compute_deterministic_alignment_loss(torch_embs, torch_seq_lens, torch_steps)\n",
    "carl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9479, -1.0044, -0.8491,  ..., -1.0135, -1.0736, -1.0281],\n",
       "        [-0.9675, -0.9829, -0.8366,  ..., -1.0082, -1.0672, -1.0317],\n",
       "        [-0.9816, -1.0059, -0.8138,  ..., -1.0172, -1.0688, -1.0307],\n",
       "        ...,\n",
       "        [-0.9337, -0.9292, -1.0141,  ..., -0.8647, -0.9082, -1.1305],\n",
       "        [-0.9356, -0.9366, -1.0120,  ..., -0.8850, -0.8784, -1.1281],\n",
       "        [-0.9348, -0.9297, -1.0137,  ..., -0.8819, -0.9031, -1.0956]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_logits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9479, -1.0044, -0.8491,  ..., -1.0135, -1.0736, -1.0281],\n",
       "        [-0.9675, -0.9829, -0.8366,  ..., -1.0082, -1.0672, -1.0317],\n",
       "        [-0.9816, -1.0059, -0.8138,  ..., -1.0172, -1.0688, -1.0307],\n",
       "        ...,\n",
       "        [-0.9337, -0.9292, -1.0141,  ..., -0.8647, -0.9082, -1.1305],\n",
       "        [-0.9356, -0.9366, -1.0120,  ..., -0.8850, -0.8784, -1.1281],\n",
       "        [-0.9348, -0.9297, -1.0137,  ..., -0.8819, -0.9031, -1.0956]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carl_logits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carl",
   "language": "python",
   "name": "carl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the structor is ResNet50 -> Transformation -> Position Encoding -> Transformer Encoder-> MLP head\n",
    "## (B , T , 3 , 224 , 224) -> (B , T , 1024 , 14 , 14) -> (B, T , 2048, 7 , 7) \n",
    "## -> (B * T , 2048 , 1 , 1) \n",
    "## -> (B*T , 2048 ) -> (B*T , 256) -> (B , T , 256) -> (B , T , 256) -> (B , T , 256) -> (B , T , 256) -> (B*T , 256) \n",
    "## -> (B * T , 128) -> (B , T , 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class ResNet50(nn.Module):\n",
    "    \"\"\"\n",
    "    The resnet50 layer in the carl paper, the resnet is used to extract feature. We freeze all layers prior to -3 \n",
    "        then train the following layers for our use.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Download the pretrain model.\n",
    "        Specify layers to use\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.model.children())[:-3])\n",
    "        self.finetune_layer = nn.Sequential(*list(self.model.children())[-3])\n",
    "        \n",
    "        self.resnet_pool = nn.AdaptiveAvgPool2d(1)\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Use Resnet 50 to extract per-frame features.\n",
    "        -------\n",
    "        Input:\n",
    "            x: (B , T , 3 , 224 , 224)\n",
    "        Output:\n",
    "            out: (B , T , 2048) (The output dimension is the same as the -2 layer of ResNet, the only different\n",
    "            is that we finetune layers between ResNet[-3:-1])\n",
    "        \"\"\"\n",
    "        B , T , C , W , H = x.shape\n",
    "        frames_per_batch = 25 ## Configuration of how many frames resnet can take once.\n",
    "        num_blocks = int(math.ceil(float(T) / frames_per_batch))\n",
    "        output = []\n",
    "        for i in range(num_blocks): \n",
    "            ## make sure the boundary case is considered\n",
    "            if (i+1)*frames_per_batch > T:\n",
    "                processing = x[:, i*frames_per_batch:]\n",
    "            else:\n",
    "                processing = x[:, i*frames_per_batch:(i+1)*frames_per_batch]\n",
    "            print(processing.shape)\n",
    "            processing = processing.contiguous().view(-1,C,W,H)\n",
    "            print(processing.shape)\n",
    "            ## feed into ResNet\n",
    "            self.backbone.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                resnet_frame = self.backbone(processing)\n",
    "            ## append finetune part\n",
    "            finetune_frame = self.finetune_layer(resnet_frame)\n",
    "            \n",
    "            processing = finetune_frame.contiguous().view(B,-1,2048,7,7)\n",
    "            \n",
    "            output.append(processing)\n",
    "        x = torch.cat(output,dim=1)\n",
    "        x = self.resnet_pool(x)\n",
    "        x = x.flatten(start_dim=2)\n",
    "        return x\n",
    "            \n",
    "r = ResNet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c1l1mo/.local/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 66, 3, 224, 224])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import Resize,functional\n",
    "x,_,_ = read_video(\"/home/c1l1mo/datasets/ACM_skating/Axel_and_Axel_com/467204328706015300.mp4\")\n",
    "y,_,_ = read_video(\"/home/c1l1mo/datasets/ACM_skating/Axel_and_Axel_com/467204328706015300.mp4\")\n",
    "x = torch.tensor(torch.cat((x.unsqueeze(0),y.unsqueeze(0)),dim=0)).float()\n",
    "x = x[:,:,:224,:224]\n",
    "x = x.permute(0,1,4,2,3)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 25, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([2, 25, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([2, 16, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([2, 66, 2048])\n"
     ]
    }
   ],
   "source": [
    "resnet_out = r(x)\n",
    "print(resnet_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_layer = []\n",
    "        \n",
    "        inchannel = 2048\n",
    "        for layer in range(2):\n",
    "            self.fc_layer.append(nn.Dropout(0.1))\n",
    "            self.fc_layer.append(nn.Linear(inchannel,512))\n",
    "            self.fc_layer.append(nn.BatchNorm1d(512))\n",
    "            self.fc_layer.append(nn.ReLU(True))\n",
    "            inchannel = 512\n",
    "        self.fc_layer = nn.Sequential(*self.fc_layer)\n",
    "        \n",
    "        self.video_emb = nn.Linear(512,256)\n",
    "    def forward(self,x):\n",
    "        B , T , R = x.shape\n",
    "        x = x.view(-1,R)\n",
    "        x = self.fc_layer(x)\n",
    "        \n",
    "        x = self.video_emb(x)\n",
    "        x = x.view(B,T , x.shape[1])\n",
    "        return x\n",
    "    \n",
    "t = Transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 66, 256])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = t(resnet_out)\n",
    "transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 66, 256]) torch.Size([1, 66, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 66, 256])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoder,self).__init__() ## There is NO DIFFERENCES b/t super() and super(className,self) AFTER python3\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "    def generate_position_encoding(self,seq_len,d_model):\n",
    "        \"\"\"\n",
    "        Position Encoding:\n",
    "            Generate multiple seq, the dimesion will be (seq_len,d_model)\n",
    "            For even number of dimension \"d_model\", generate sin wave.\n",
    "            For odd numbers, generate cosine wave.\n",
    "        \"\"\"\n",
    "        pos_matrix = np.zeros((seq_len,d_model))\n",
    "        for pos in range(seq_len):\n",
    "            for i in np.arange(d_model/2):\n",
    "                pos_matrix[pos,int(2*i)] = np.sin(pos / 10000**(2*i / d_model))\n",
    "                pos_matrix[pos,int(2*i)+1] = np.cos(pos / 10000**(2*i / d_model))\n",
    "        return torch.from_numpy(pos_matrix).unsqueeze(0)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        B , T , D = x.shape\n",
    "        pos_matrix = self.generate_position_encoding(T,D)\n",
    "        print(x.shape,pos_matrix.shape)\n",
    "        x = x + pos_matrix.type_as(x)\n",
    "        x = self.drop_out(x)\n",
    "        return x\n",
    "\n",
    "PE = PositionalEncoder()\n",
    "pe = PE(transformed)\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer's encoding layer, in each layer it will do attention mechanism and feedforward\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carl",
   "language": "python",
   "name": "carl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
